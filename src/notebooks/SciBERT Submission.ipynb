{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "linear-upgrade",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T15:51:19.294670Z",
     "iopub.status.busy": "2021-06-10T15:51:19.294042Z",
     "iopub.status.idle": "2021-06-10T15:51:27.473359Z",
     "shell.execute_reply": "2021-06-10T15:51:27.474027Z",
     "shell.execute_reply.started": "2021-06-10T15:25:01.071741Z"
    },
    "papermill": {
     "duration": 8.199351,
     "end_time": "2021-06-10T15:51:27.474343",
     "exception": false,
     "start_time": "2021-06-10T15:51:19.274992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm,trange\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "import torch\n",
    "from transformers import BertForTokenClassification, AdamW, BertTokenizerFast\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "#from rich.console import Console\n",
    "#from rich.progress import track\n",
    "from tqdm import tqdm\n",
    "from transformers import *\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expensive-necklace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T15:51:27.540385Z",
     "iopub.status.busy": "2021-06-10T15:51:27.539584Z",
     "iopub.status.idle": "2021-06-10T15:51:27.542778Z",
     "shell.execute_reply": "2021-06-10T15:51:27.543346Z",
     "shell.execute_reply.started": "2021-06-10T15:25:11.552350Z"
    },
    "papermill": {
     "duration": 0.060579,
     "end_time": "2021-06-10T15:51:27.543522",
     "exception": false,
     "start_time": "2021-06-10T15:51:27.482943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-ancient",
   "metadata": {
    "papermill": {
     "duration": 0.007253,
     "end_time": "2021-06-10T15:51:27.559257",
     "exception": false,
     "start_time": "2021-06-10T15:51:27.552004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission from saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "offensive-seeker",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T15:51:27.577660Z",
     "iopub.status.busy": "2021-06-10T15:51:27.577014Z",
     "iopub.status.idle": "2021-06-10T15:51:27.580031Z",
     "shell.execute_reply": "2021-06-10T15:51:27.579500Z",
     "shell.execute_reply.started": "2021-06-10T15:50:19.680492Z"
    },
    "papermill": {
     "duration": 0.013416,
     "end_time": "2021-06-10T15:51:27.580150",
     "exception": false,
     "start_time": "2021-06-10T15:51:27.566734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_path  = '../input/coleridgeinitiative-show-us-the-data/test'\n",
    "model_path = '../input/show-us-the-data-bert-weights/SciBERT_Hyperparams/SciBERT_Hyperparams/SciBERT_Finetuned_5Eps' \n",
    "tokenizer_path = '../input/tokenizers' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "natural-interest",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T15:51:27.598634Z",
     "iopub.status.busy": "2021-06-10T15:51:27.598164Z",
     "iopub.status.idle": "2021-06-10T15:51:41.516592Z",
     "shell.execute_reply": "2021-06-10T15:51:41.515386Z",
     "shell.execute_reply.started": "2021-06-10T15:50:19.911157Z"
    },
    "papermill": {
     "duration": 13.929114,
     "end_time": "2021-06-10T15:51:41.516749",
     "exception": false,
     "start_time": "2021-06-10T15:51:27.587635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\n",
    "    model_path, \n",
    "    num_labels=3, \n",
    "    output_attentions=False, \n",
    "    output_hidden_states=False\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_path, \n",
    "    do_lower_case=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "significant-watershed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T15:51:41.551983Z",
     "iopub.status.busy": "2021-06-10T15:51:41.540536Z",
     "iopub.status.idle": "2021-06-10T15:51:41.567427Z",
     "shell.execute_reply": "2021-06-10T15:51:41.567025Z",
     "shell.execute_reply.started": "2021-06-10T15:50:29.458320Z"
    },
    "papermill": {
     "duration": 0.041519,
     "end_time": "2021-06-10T15:51:41.567534",
     "exception": false,
     "start_time": "2021-06-10T15:51:41.526015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SubmitPred:\n",
    "\n",
    "    def __init__(self, test_path, model, tokenizer, batch_size=128, paper_batch_size=1024):\n",
    "        self.test_path = test_path\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.MAX_LENGTH = 64  # max no. words for each sentence.\n",
    "        self.OVERLAP = 20\n",
    "        self.tag2str = {2: 'O', 1: 'B', 0: 'I'}\n",
    "        self.batch_size = batch_size\n",
    "        self.paper_batch_size = paper_batch_size\n",
    "\n",
    "\n",
    "    def tokenize_sent(self, sentence):\n",
    "        tokenized_sentence = []\n",
    "        sentence = sentence.split()\n",
    "        for word in sentence:\n",
    "            tokenized_word = self.tokenizer.tokenize(word)\n",
    "            tokenized_sentence.extend(tokenized_word)\n",
    "        return tokenized_sentence\n",
    "\n",
    "    def read_and_create_csv(self):\n",
    "        all_test_papers = os.listdir(self.test_path)\n",
    "        self.submission = pd.DataFrame({'Id': [paper.split('.')[0] for paper in all_test_papers], \"PredictionString\": \"\"})\n",
    "\n",
    "    @staticmethod\n",
    "    def shorten_sentences(sentences, max_len, overlap):\n",
    "        short_sentences = []\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            if len(words) > max_len:\n",
    "                for p in range(0, len(words), max_len - overlap):\n",
    "                    short_sentences.append(' '.join(words[p:p + max_len]))\n",
    "            else:\n",
    "                short_sentences.append(sentence)\n",
    "        return short_sentences\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_training_text(txt):\n",
    "        return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n",
    "\n",
    "    def add_padding(self, tokenized_sentences):\n",
    "        return pad_sequences(\n",
    "            tokenized_sentences,\n",
    "            value='[PAD]',\n",
    "            dtype=object,\n",
    "            maxlen=self.MAX_LENGTH,\n",
    "            truncating='post',\n",
    "            padding='post')\n",
    "\n",
    "    @staticmethod\n",
    "    def get_attention_mask(input_ids, ignore_tokens=[0, 101, 102]):\n",
    "        return list(map(lambda sent: list(map(lambda token: float(token not in ignore_tokens), sent)), input_ids))\n",
    "\n",
    "    @staticmethod\n",
    "    def jaccard_similarity(list1, list2):\n",
    "        intersection = len(list(set(list1).intersection(list2)))\n",
    "        union = (len(list1) + len(list2)) - intersection\n",
    "        return float(intersection) / union\n",
    "\n",
    "    @staticmethod\n",
    "    def add_start_end_tokens(tupled_sentence):\n",
    "        tupled_sentence.insert(0, '[CLS]')\n",
    "        tupled_sentence.append('[SEP]')\n",
    "        return tupled_sentence\n",
    "\n",
    "    def read_papers(self, i_batch):\n",
    "        paper_length = []\n",
    "        sentences_e = []\n",
    "        papers = {}\n",
    "        for paper_id in self.submission['Id'][i_batch * self.paper_batch_size: min(len(self.submission['Id']), (i_batch + 1) * self.paper_batch_size)]:\n",
    "            with open(f'{self.test_path}/{paper_id}.json', 'r') as f:\n",
    "                paper = json.load(f)\n",
    "                papers[paper_id] = paper\n",
    "            sentences = set(\n",
    "                [self.clean_training_text(sentence) for section in paper for sentence in section['text'].split('.')])\n",
    "            sentences = self.shorten_sentences(sentences, self.MAX_LENGTH, self.OVERLAP)\n",
    "            sentences = [sentence for sentence in sentences if len(sentence) > 10]\n",
    "            sentences_e.extend(sentences)\n",
    "            paper_length.append(len(sentences))\n",
    "        \n",
    "        return papers, paper_length, sentences_e\n",
    "    \n",
    "\n",
    "    def no_jaccard_overlap(self, filter_predictions, prediction):\n",
    "        return not any(map(lambda filter_prediction: self.jaccard_similarity(filter_prediction.split(), prediction.split()) > 0.70, filter_predictions))\n",
    "    \n",
    "    def jaccard_filter(self, predictions):\n",
    "        if len(predictions) == 0:\n",
    "            return []\n",
    "        if len(predictions) == 1:\n",
    "            return predictions\n",
    "        \n",
    "        filt = [predictions[0]]\n",
    "        for pred in predictions[1:]:\n",
    "            if self.no_jaccard_overlap(filt, pred):\n",
    "                filt.append(pred)\n",
    "                \n",
    "        return filt\n",
    "    \n",
    "    def run(self):\n",
    "        #self.load_submission()\n",
    "        self.read_and_create_csv()\n",
    "        \n",
    "        for i_batch in range(math.ceil(len(self.submission)/self.paper_batch_size)):\n",
    "            #print(\"Reading papers\")\n",
    "            papers, paper_length, sentences_e = self.read_papers(i_batch)\n",
    "\n",
    "            #print(\"Tokenizing papers\")\n",
    "            # Move padding, tokenization and start end into one comprehension\n",
    "            padded_sentences = self.add_padding(list(map(lambda sentence: self.add_start_end_tokens(self.tokenize_sent(sentence)), sentences_e)))\n",
    "            input_ids = list(map(lambda text: self.tokenizer.convert_tokens_to_ids(text), padded_sentences))\n",
    "            del padded_sentences\n",
    "            attention_mask = self.get_attention_mask(input_ids, ignore_tokens=[0])\n",
    "\n",
    "            predicts = torch.tensor(input_ids, requires_grad=False).to(device)\n",
    "            masks = torch.tensor(attention_mask, requires_grad=False).to(device)\n",
    "\n",
    "            del attention_mask\n",
    "\n",
    "            #print(\"Datasets\")\n",
    "            predict_data = TensorDataset(predicts, masks)\n",
    "            predict_dataloader = DataLoader(predict_data, batch_size=self.batch_size)\n",
    "\n",
    "            all_predictions = torch.empty((0, self.MAX_LENGTH, 3), device=device, requires_grad=False)\n",
    "\n",
    "            #print(\"Model inference\")\n",
    "            for step, batch in enumerate(predict_dataloader):\n",
    "                b_input_ids, b_input_mask = batch\n",
    "                with torch.no_grad():\n",
    "                    output = self.model(b_input_ids, attention_mask=b_input_mask)\n",
    "                all_predictions = torch.vstack((all_predictions, output[0]))\n",
    "\n",
    "            all_predictions = np.argmax(all_predictions.to('cpu').numpy(), axis=2)\n",
    "\n",
    "            del predicts, masks\n",
    "\n",
    "            #print(\"Big loops\")\n",
    "            all_preds_str = list(map(lambda pred: map(lambda token: self.tag2str[token], pred), all_predictions))\n",
    "            #all_preds_str = [[self.tag2str[token] for token in pred] for pred in all_predictions]\n",
    "            all_sent_int = input_ids\n",
    "            final_predics = []\n",
    "\n",
    "            for pap_len in paper_length:\n",
    "                labels = []\n",
    "                for sentence, pred in zip(all_sent_int[:pap_len], all_preds_str[:pap_len]):\n",
    "                    phrase = []\n",
    "                    phrase_test = []\n",
    "                    for word, tag in zip(sentence, pred):\n",
    "\n",
    "                        if tag == \"I\" or tag == \"B\":\n",
    "                            phrase_test.append(word)\n",
    "                            if word != 0 and word != 101 and word != 102:\n",
    "                                phrase.append(word)\n",
    "                        else:\n",
    "                            if len(phrase) != 0:\n",
    "                                labels.append(self.tokenizer.decode(phrase))\n",
    "                                phrase_test = []\n",
    "                                phrase = []\n",
    "\n",
    "                final_predics.append(labels)\n",
    "                del all_sent_int[:pap_len], all_preds_str[:pap_len]\n",
    "   \n",
    "            final_predics = [[pred for pred in preds if not pred.startswith(\"#\")] for preds in final_predics]\n",
    "\n",
    "            #print(\"Big loops 2\")\n",
    "            filtered = [self.jaccard_filter(predictions) for predictions in final_predics]\n",
    "            \n",
    "            filtered = [\"|\".join(filt) if len(filt) != 0 else \"\" for filt in filtered]\n",
    "            self.submission['PredictionString'][i_batch * self.paper_batch_size: min(len(self.submission['Id']), (i_batch + 1) * self.paper_batch_size)] = filtered\n",
    "            # self.submission['PredictionString'] = self.submission.apply(lambda x:\"|\".join(x.PredictionString),axis=1)\n",
    "\n",
    "    def save_csv(self):\n",
    "        self.submission.to_csv(f'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "noted-sensitivity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T15:51:41.585915Z",
     "iopub.status.busy": "2021-06-10T15:51:41.585201Z",
     "iopub.status.idle": "2021-06-10T15:51:41.587487Z",
     "shell.execute_reply": "2021-06-10T15:51:41.587923Z",
     "shell.execute_reply.started": "2021-06-10T15:50:29.501995Z"
    },
    "papermill": {
     "duration": 0.013137,
     "end_time": "2021-06-10T15:51:41.588045",
     "exception": false,
     "start_time": "2021-06-10T15:51:41.574908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_pred = SubmitPred(\n",
    "    test_path,\n",
    "    model,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "prescription-issue",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T15:51:41.605957Z",
     "iopub.status.busy": "2021-06-10T15:51:41.605414Z",
     "iopub.status.idle": "2021-06-10T15:51:48.749899Z",
     "shell.execute_reply": "2021-06-10T15:51:48.749399Z",
     "shell.execute_reply.started": "2021-06-10T15:50:29.524548Z"
    },
    "papermill": {
     "duration": 7.154679,
     "end_time": "2021-06-10T15:51:48.750028",
     "exception": false,
     "start_time": "2021-06-10T15:51:41.595349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_pred.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "wrapped-offering",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T15:51:48.769041Z",
     "iopub.status.busy": "2021-06-10T15:51:48.768313Z",
     "iopub.status.idle": "2021-06-10T15:51:48.890833Z",
     "shell.execute_reply": "2021-06-10T15:51:48.890302Z",
     "shell.execute_reply.started": "2021-06-10T15:50:37.720718Z"
    },
    "papermill": {
     "duration": 0.13337,
     "end_time": "2021-06-10T15:51:48.890969",
     "exception": false,
     "start_time": "2021-06-10T15:51:48.757599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_pred.save_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 38.625521,
   "end_time": "2021-06-10T15:51:51.609075",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-10T15:51:12.983554",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
