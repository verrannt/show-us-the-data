{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# For better error messages when 'device side assert'\n!export CUDA_LAUNCH_BLOCKING=1\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2021-06-08T20:35:42.447265Z","iopub.execute_input":"2021-06-08T20:35:42.447691Z","iopub.status.idle":"2021-06-08T20:35:43.232343Z","shell.execute_reply.started":"2021-06-08T20:35:42.447654Z","shell.execute_reply":"2021-06-08T20:35:43.230889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\nimport json\nimport re\nimport os\nimport pandas as pd\nimport pickle\nimport math\nfrom tqdm import tqdm,trange\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import Progbar\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score,accuracy_score\nimport torch\nfrom transformers import BertForTokenClassification, AdamW, BertTokenizerFast, AutoModelForTokenClassification, AutoTokenizer\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,WeightedRandomSampler\n#from rich.console import Console\n#from rich.progress import track\nfrom tqdm import tqdm\nfrom transformers import BertTokenizerFast,BertForTokenClassification\n\nimport random","metadata":{"execution":{"iopub.status.busy":"2021-06-08T20:35:46.40547Z","iopub.execute_input":"2021-06-08T20:35:46.40581Z","iopub.status.idle":"2021-06-08T20:35:55.541798Z","shell.execute_reply.started":"2021-06-08T20:35:46.405776Z","shell.execute_reply":"2021-06-08T20:35:55.540723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\nprint(f'Using {device} device')","metadata":{"execution":{"iopub.status.busy":"2021-06-08T20:35:55.543619Z","iopub.execute_input":"2021-06-08T20:35:55.544042Z","iopub.status.idle":"2021-06-08T20:35:55.601932Z","shell.execute_reply.started":"2021-06-08T20:35:55.543997Z","shell.execute_reply":"2021-06-08T20:35:55.600556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def timer(func):\n    \"\"\"\n    Record execution time of any function with timer decorator\n    Usage: just decorate a function when building it, the \n    decorator will be called every time the function is executed.\n    # build the function\n    @timer\n    def some_function(some_arg):\n        # do_something\n        return 'foo'\n        \n    # call it\n    some_function('boo')\n    # output:\n    >> Function 'some_function' finished after 0.01 seconds.\n    \"\"\"\n\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        results = func(*args, **kwargs)\n        duration = time.time() - start\n        print(\"Function '{}' finished after {:.4f} seconds.\"\\\n              .format(func.__name__, duration))\n        return results\n    return wrapper","metadata":{"execution":{"iopub.status.busy":"2021-06-08T20:35:55.604587Z","iopub.execute_input":"2021-06-08T20:35:55.605338Z","iopub.status.idle":"2021-06-08T20:35:55.614767Z","shell.execute_reply.started":"2021-06-08T20:35:55.605291Z","shell.execute_reply":"2021-06-08T20:35:55.61358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"class Pipeline:\n\n    def __init__(self, configs):\n        self.configs = configs\n        self.tokenizer = None\n\n    def set_tokenizer(self, tokenizer):\n        \"\"\"\n        Set a custom tokenizer to be used when running the pipeline.\n        If not, this will default to BERTTokenizerFast in `run()`\n        \"\"\"\n        self.tokenizer = tokenizer\n\n    def tokenize_and_preserve_labels(self, tupled_sentence):\n        tokenized_sentence = []\n        labels = []\n\n        for (word, label) in tupled_sentence:\n\n            # Tokenize the word\n            tokenized_word = self.tokenizer.tokenize(word)\n            tokenized_sentence.extend(tokenized_word)\n            \n            # Repeat the label for words that are broken up into several tokens\n            labels.extend([label]*len(tokenized_word))\n            \n        # Add the tokenized word and its label to the final tokenized word list\n        return list(zip(tokenized_sentence, labels))\n\n    def add_start_end_tokens(self, tupled_sentence):\n        tupled_sentence.insert(0, ('[CLS]', 'O'))\n        tupled_sentence.append(('[SEP]', 'O'))\n        return tupled_sentence\n\n    def add_padding(self, tokenized_sentences, labels):\n        # Note that this implicitly converts to an array of objects (strings)\n        \n        padded_sentences = pad_sequences(\n            tokenized_sentences, \n            value='[PAD]', \n            dtype=object, \n            maxlen=self.configs.MAX_LENGTH, \n            truncating='post', \n            padding='post')\n\n        padded_labels = pad_sequences(\n            labels, \n            value='O', \n            dtype=object, \n            maxlen=self.configs.MAX_LENGTH, \n            truncating='post', \n            padding='post')\n        \n        return padded_sentences, padded_labels\n\n    def get_attention_mask(self, input_ids, ignore_tokens=[0,101,102]):\n        \"\"\"\n        Compute the attention marks for the tokens in `input_ids`, which is\n        assumed to be a list (batch) of lists (sentences) of integer tokens.\n        Tokens that should be masked out can be specified using the \n        `ignore_tokens` parameter. By default, these are supposed to be 0, 101,\n        and 102, representing [PAD], [CLS], and [SEP] tokens, respectively.\n        \"\"\"\n\n        return [\n            [ float(token not in ignore_tokens) for token in sent ] \n                for sent in input_ids\n        ]\n\n    def run(self, ner_data):\n        \"\"\"\n        Run extracted sentence data through the pipeline.\n        \"\"\"\n\n        console = Console()\n\n        # Initialize tokenizer\n        if not self.tokenizer:\n            self.tokenizer = BertTokenizerFast.from_pretrained(\n                'bert-base-cased', do_lower_case=False)\n            console.log('Initialized default BERT tokenizer')\n        else:\n            console.log('Using custom tokenizer')\n\n        # Tokenize into known tokens\n        ner_data = [\n            self.tokenize_and_preserve_labels(sentence) for sentence in \n                track(ner_data, description='Tokenizing words...')\n        ]\n        console.log('Tokenized words')\n\n        with console.status(\"[bold green]Running pipeline...\") as status:\n\n            # Add [CLS] and [SEP] tokens to beginning and end\n            ner_data = [\n                self.add_start_end_tokens(sentence)\n                    for sentence in ner_data\n            ]\n            console.log('Added [CLS] and [SEP] tokens')\n\n            # Get only sentences, not labels\n            tokenized_sentences = [\n                [token_label_tuple[0] for token_label_tuple in sent]\n                    for sent in ner_data\n            ]\n\n            # Get only labels, not sentences\n            labels = [\n                [token_label_tuple[1] for token_label_tuple in sent] \n                    for sent in ner_data \n            ]\n\n            # Pad sentences and labels \n            padded_sentences, padded_labels = self.add_padding(\n                tokenized_sentences, labels)\n            console.log('Padded sentences and labels')\n\n            # Convert to integer ids\n            input_ids = [\n                self.tokenizer.convert_tokens_to_ids(text) \n                    for text in padded_sentences\n            ]\n            tags = [\n                self.tokenizer.convert_tokens_to_ids(text) \n                    for text in padded_labels\n            ]\n            console.log('Converted to integer ids')\n\n            # Compute attention mask from input tokens\n            attention_mask = self.get_attention_mask(\n                input_ids,\n                # Only ignore [PAD] tokens (integer 0)\n                ignore_tokens=[0]\n            )\n            \n            console.log('Computed attention mask')\n\n        if self.configs.SAVE:\n            ParseUtils.save_file(\n                {\n                    'input_ids': input_ids, \n                    'tags': tags,\n                    'attention_mask': attention_mask\n                },\n                self.configs.DATA_PATH,\n                self.configs.TOKENIZED_FILENAME\n            )\n\n        return input_ids, tags, attention_mask\n\n    def load_outputs(self):\n        \"\"\"\n        Recover the outputs of a previously completed run from storage.\n        \"\"\"\n        output_dict = ParseUtils.load_file(\n            self.configs.DATA_PATH,\n            self.configs.TOKENIZED_FILENAME,\n        )\n\n        return output_dict['input_ids'], \\\n               output_dict['tags'], \\\n               output_dict['attention_mask']\n\n    def extract(self):\n        ner_data = ParseUtils.extract(\n            max_len = self.configs.MAX_LENGTH,\n            overlap = self.configs.OVERLAP,\n            max_sample = self.configs.MAX_SAMPLE,\n            max_text_tokens = self.configs.MAX_TEXT_TOKENS,\n            train_df_path = self.configs.TRAIN_DF_PATH,\n            train_data_path = self.configs.TRAIN_DATA_PATH,\n            ignore_label_case = self.configs.IGNORE_LABEL_CASE,\n            exclude_non_exact_label_match = self.configs.EXCLUDE_NON_EXACT_LABEL_MATCH\n        )\n\n        # Write data to file\n        if self.configs.SAVE:\n            ParseUtils.save_extracted(\n                ner_data, \n                self.configs.DATA_PATH, \n                self.configs.EXTRACTED_FILENAME\n            )\n\n        return ner_data\n\n    def load_extracted(self):\n        return ParseUtils.load_extracted(\n            self.configs.DATA_PATH, \n            self.configs.EXTRACTED_FILENAME\n        )\n\n\nclass PipelineConfigs:\n\n    def __init__(\n        self,\n        DATA_PATH,\n        SAVE,\n        EXTRACTED_FILENAME,\n        TOKENIZED_FILENAME,\n        MAX_SAMPLE,\n        MAX_LENGTH = 64,\n        OVERLAP = 20,\n        MAX_TEXT_TOKENS=200000,\n        IGNORE_LABEL_CASE=True,\n        EXCLUDE_NON_EXACT_LABEL_MATCH=True\n    ):\n\n        # Maximum number of words for each sentence\n        self.MAX_LENGTH = MAX_LENGTH\n\n        # If a sentence exceeds MAX_LENGTH, we split it to multiple sentences \n        # with overlapping\n        self.OVERLAP = OVERLAP\n\n        # During development, you may want to only load part of the data. Leave\n        # uninitialized during production\n        self.MAX_SAMPLE = MAX_SAMPLE\n\n        self.DATA_PATH = DATA_PATH\n        #self.DATA_PATH = \\\n        #    os.path.join(\n        #        os.path.join(\n        #            os.path.dirname(\n        #                os.path.dirname(\n        #                    os.path.dirname(\n        #                        os.path.dirname(__file__)\n        #                    )\n        #                )\n        #            ),\n        #            'data'\n        #        ), \n        #        'coleridgeinitiative-show-us-the-data'\n        #    )\n        self.TRAIN_DATA_PATH = os.path.join(self.DATA_PATH, 'train')\n        self.TRAIN_DF_PATH = os.path.join(self.DATA_PATH, 'train.csv')\n        self.TEST_DATA_PATH = os.path.join(self.DATA_PATH, 'test')\n\n        # If SAVE is true, will save the extracted and/or the tokenized data\n        # under the provided filename(s)\n        self.SAVE = SAVE\n        self.EXTRACTED_FILENAME = EXTRACTED_FILENAME\n        self.TOKENIZED_FILENAME = TOKENIZED_FILENAME\n        # Maximum amount of tokens in training texts. Longer texts will be discarded\n        self.MAX_TEXT_TOKENS = MAX_TEXT_TOKENS\n        # Whether the tagger should ignore the case of the label when matching labels to the text\n        self.IGNORE_LABEL_CASE = IGNORE_LABEL_CASE\n        # Whether to exclude texts that do not have a single one-on-one (case insensitve) label match\n        self.EXCLUDE_NON_EXACT_LABEL_MATCH = EXCLUDE_NON_EXACT_LABEL_MATCH\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T20:35:55.616644Z","iopub.execute_input":"2021-06-08T20:35:55.61711Z","iopub.status.idle":"2021-06-08T20:35:55.648918Z","shell.execute_reply.started":"2021-06-08T20:35:55.617064Z","shell.execute_reply":"2021-06-08T20:35:55.647846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ParseUtils:\n\n    @staticmethod\n    def count_in_json(json_id, label, train_data_path):\n        path_to_json = os.path.join(train_data_path, (json_id + '.json'))\n        count_dict = {}\n        with open(path_to_json, 'r') as f:\n            json_decode = json.load(f)\n            for data in json_decode:\n                heading = data.get('section_title')\n                content = data.get('text')\n                count_dict[heading] = content.count(heading)\n        return count_dict\n\n    @staticmethod\n    def shorten_sentences(sentences, max_len, overlap):\n        short_sentences = []\n        for sentence in sentences:\n            words = sentence.split()\n            if len(words) > max_len:\n                for p in range(0, len(words), max_len - overlap):\n                    short_sentences.append(' '.join(words[p:p + max_len]))\n            else:\n                short_sentences.append(sentence)\n        return short_sentences\n\n    @staticmethod\n    def clean_training_text(txt):\n        \"\"\"\n        similar to the default clean_text function but without lowercasing.\n        \"\"\"\n        txt = re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n\n        return txt\n\n    @staticmethod\n    def find_sublist(big_list, small_list):\n        all_positions = []\n        for i in range(len(big_list) - len(small_list) + 1):\n            if small_list == big_list[i:i + len(small_list)]:\n                all_positions.append(i)\n\n        return all_positions\n\n    @staticmethod\n    def tag_sentence(sentence, labels, ignore_case):  # requirement: both sentence and\n        re_flags = re.IGNORECASE if ignore_case else None\n\n        # labels are already cleaned\n        sentence_words = sentence.split()\n\n        if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence,\n                                                 flags=re_flags) for label in labels):  # positive sample\n            nes = ['O'] * len(sentence_words)\n            for label in labels:\n                label_words = label.split()\n\n                if ignore_case:\n                    nocase_label_words = list(map(lambda word: word.lower(), label_words))\n                    nocase_sentence_words = list(map(lambda word: word.lower(), sentence_words))\n                    all_pos = ParseUtils.find_sublist(nocase_sentence_words, nocase_label_words)\n                else:\n                    all_pos = ParseUtils.find_sublist(sentence_words, label_words)\n\n                for pos in all_pos:\n                    nes[pos] = 'B'\n                    for i in range(pos + 1, pos + len(label_words)):\n                        nes[i] = 'I'\n\n            return True, list(zip(sentence_words, nes))\n\n        else:  # negative sample\n            nes = ['O'] * len(sentence_words)\n            return False, list(zip(sentence_words, nes))\n\n    @staticmethod\n    def read_append_return(filename, train_data_path, output='text'):\n        \"\"\"\n        Function to read json file and then return the text data from them and append to the dataframe\n\n        Basically parse json but then from https://www.kaggle.com/prashansdixit/coleridge-initiative-eda-baseline-model\n        \"\"\"\n        json_path = os.path.join(train_data_path, (filename + '.json'))\n        headings = []\n        contents = []\n        combined = []\n        with open(json_path, 'r') as f:\n            json_decode = json.load(f)\n            for data in json_decode:\n                headings.append(data.get('section_title'))\n                contents.append(data.get('text'))\n                combined.append(data.get('section_title'))\n                combined.append(data.get('text'))\n\n        all_headings = ' '.join(headings)\n        all_contents = ' '.join(contents)\n        all_data = '. '.join(combined)\n\n        if output == 'text':\n            return all_contents\n        elif output == 'head':\n            return all_headings\n        else:\n            return all_data\n\n    @staticmethod\n    def save_extracted(ner_data, data_path, file_name):\n        with open(os.path.join(data_path, file_name), 'w') as f:\n            for row in ner_data:\n                words, nes = list(zip(*row))\n                row_json = {'tokens': words, 'tags': nes}\n                json.dump(row_json, f)\n                f.write('\\n')\n\n    @staticmethod\n    def load_extracted(data_path, file_name):\n        ner_data = []\n        with open(os.path.join(data_path, file_name), 'r') as f:\n            for line in f.readlines():\n                # Each line is formatted in JSON format, e.g.\n                # { \"tokens\" : [\"A\", \"short\", \"sentence\"],\n                #   \"tags\"   : [\"0\", \"0\", \"0\"] }\n                sentence = json.loads(line)\n\n                # From the tokens and tags, we create a list of\n                # tuples of the form\n                # [ (\"A\", \"0\"), (\"short\", \"0\"), (\"sentence\", \"0\")]\n                sentence_tuple_list = [\n                    (token, tag) for token, tag\n                    in zip(sentence[\"tokens\"], sentence[\"tags\"])\n                ]\n\n                # Each of these parsed sentences becomes an entry\n                # in our overall data list\n                ner_data.append(sentence_tuple_list)\n\n        return ner_data\n\n    @staticmethod\n    def load_auxiliary_datasets(data_path, file_name):\n        with open(os.path.join(data_path, file_name), 'r') as f:\n            return f.read().split('\\n')\n\n    @staticmethod\n    def load_tokenized_auxiliary_datasets(data_path, file_name):\n        with open(os.path.join(data_path, file_name), 'r') as f:\n            datasets = f.read().split('\\n')\n            return [[int(item) for item in row.split(',') if item != ''] for row in datasets if len(row) > 0]\n\n    @staticmethod\n    def save_file(output, data_path, file_name):\n        with open(os.path.join(data_path, file_name), 'wb') as f:\n            pickle.dump(output, f)\n\n    @staticmethod\n    def load_file(data_path, file_name):\n        with open(os.path.join(data_path, file_name), 'rb') as f:\n            output = pickle.load(f)\n        return output\n\n    @staticmethod\n    def all_labels_mentioned(data):\n        \"\"\"\n        Method that can be applied to a dataframe and check, for all dataset labels, if they occur in the text at least\n        once. Case insensitive\n        \"\"\"\n        labels = data['dataset_label'].split(\"|\")\n        return all(list(map(lambda label: data['text'].lower().count(label.lower()) > 0, labels)))\n\n    @staticmethod\n    def extract(\n            max_len,\n            overlap,\n            max_sample,\n            max_text_tokens,\n            train_df_path,\n            train_data_path,\n            ignore_label_case,\n            exclude_non_exact_label_match\n\n    ):\n        \"\"\"\n        Reads the training data from storage using the train.csv file as well\n        as all json files inside the train folder, and computes a list,\n        where each element is a sentence. Each sentence is itself a list,\n        consisting of tuples, where the first element is the word (token) and\n        the second is the label (tag).\n\n        This is an example of the data list returned:\n\n            ner_data = [\n                ...\n                [\n                    (\"This\", \"0\"),\n                    (\"is\", \"0\"),\n                    (\"New\", \"LOC\"),\n                    (\"York\", \"LOC\"),\n                ],\n                ...\n            ]\n\n        If `save` is True, the data will be stored on disk in the DATA_PATH\n        directory in a single text file, where each line is in JSON format, e.g.\n\n            { \"tokens\" : [\"A\", \"short\", \"sentence\"], \"tags\" : [\"0\", \"0\", \"0\"] }\n        \"\"\"\n\n        # Read data in CSV file\n        train = pd.read_csv(train_df_path)\n        train = train[:max_sample]\n        print(f'Found {len(train)} raw training rows')\n\n        # Group rows by publication ID\n        train = train.groupby('Id').agg({\n            'pub_title': 'first',\n            'dataset_title': '|'.join,\n            'dataset_label': '|'.join,\n            'cleaned_label': '|'.join\n        }).reset_index()\n        print(f'Found {len(train)} unique training rows')\n\n        print('Loading texts, this might take a while...')\n        # Read texts for text length analysis\n        train['text'] = train['Id'].apply(lambda ID: ParseUtils.read_append_return(ID, train_data_path))\n        train['text_token_length'] = train['text'].apply(lambda text: len(text))\n\n        # Remove texts that have more tokens than max_text_tokens\n        train = train[train['text_token_length'] <= max_text_tokens]\n        print(f'Removed texts exceeding max length, {len(train)} training rows left')\n\n        if exclude_non_exact_label_match:\n            # Count label mentions in text\n            train[\"all_labels_mentioned\"] = train.apply(ParseUtils.all_labels_mentioned, axis=1)\n\n            # Remove texts that have 0 label count for at least 1 label\n            train = train[train['all_labels_mentioned']]\n            print(f'Removed texts that had at least one label with 0 exact (case insensitive) matches in the text, '\n                  f'{len(train)} training rows left')\n\n        # Read individual papers by ID from storage\n        papers = {}\n        for paper_id in train['Id'].unique():\n            with open(f'{train_data_path}/{paper_id}.json', 'r') as f:\n                paper = json.load(f)\n                papers[paper_id] = paper\n\n        cnt_pos, cnt_neg = 0, 0  # number of sentences that contain/not contain labels\n        ner_data = []\n\n        pbar = tqdm(total=len(train))\n        for i, id, dataset_label in train[['Id', 'dataset_label']].itertuples():\n            # paper\n            paper = papers[id]\n\n            # labels\n            labels = dataset_label.split('|')\n            labels = [ParseUtils.clean_training_text(label) for label in labels]\n\n            # sentences\n            sentences = set([\n                ParseUtils.clean_training_text(sentence)\n                for section in paper\n                for sentence in section['text'].split('.')\n            ])\n            sentences = ParseUtils.shorten_sentences(\n                sentences, max_len, overlap)\n\n            # only accept sentences with length > 10 chars\n            sentences = [sentence for sentence in sentences if len(sentence) > 10]\n\n            # positive sample\n            for sentence in sentences:\n                is_positive, tags = ParseUtils.tag_sentence(sentence, labels, ignore_label_case)\n                if is_positive:\n                    cnt_pos += 1\n                else:\n                    cnt_neg += 1\n                ner_data.append(tags)\n\n            # process bar\n            pbar.update(1)\n            pbar.set_description(f\"Training data size: {cnt_pos} positives + {cnt_neg} negatives\")\n\n        # shuffling\n        # random.shuffle(ner_data)\n\n        return ner_data\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-08T20:35:56.683035Z","iopub.execute_input":"2021-06-08T20:35:56.683401Z","iopub.status.idle":"2021-06-08T20:35:56.731973Z","shell.execute_reply.started":"2021-06-08T20:35:56.683359Z","shell.execute_reply":"2021-06-08T20:35:56.730747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path ='../input/showusthedata-tokenized'","metadata":{"execution":{"iopub.status.busy":"2021-06-08T20:36:07.702268Z","iopub.execute_input":"2021-06-08T20:36:07.702709Z","iopub.status.idle":"2021-06-08T20:36:07.707474Z","shell.execute_reply.started":"2021-06-08T20:36:07.70266Z","shell.execute_reply":"2021-06-08T20:36:07.706235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"configs = PipelineConfigs(\n    DATA_PATH = data_path,\n    MAX_LENGTH = 64,\n    OVERLAP = 20,\n    MAX_SAMPLE = None,\n    SAVE = False,\n    EXTRACTED_FILENAME = 'train_ner.data',\n    TOKENIZED_FILENAME = 'train_ner.data.scibert-tokenized',\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T20:36:09.067421Z","iopub.execute_input":"2021-06-08T20:36:09.067841Z","iopub.status.idle":"2021-06-08T20:36:09.074902Z","shell.execute_reply.started":"2021-06-08T20:36:09.067809Z","shell.execute_reply":"2021-06-08T20:36:09.073511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline = Pipeline(configs)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:02:35.487226Z","iopub.execute_input":"2021-06-03T19:02:35.487554Z","iopub.status.idle":"2021-06-03T19:02:35.493687Z","shell.execute_reply.started":"2021-06-03T19:02:35.487521Z","shell.execute_reply":"2021-06-03T19:02:35.492917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_cased', do_lower_case=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T20:36:11.077301Z","iopub.execute_input":"2021-06-08T20:36:11.077749Z","iopub.status.idle":"2021-06-08T20:36:14.993173Z","shell.execute_reply.started":"2021-06-08T20:36:11.077701Z","shell.execute_reply":"2021-06-08T20:36:14.991736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"o_id = tokenizer.convert_tokens_to_ids(\"O\")\ni_id = tokenizer.convert_tokens_to_ids(\"I\")","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:02:36.211231Z","iopub.execute_input":"2021-06-03T19:02:36.211549Z","iopub.status.idle":"2021-06-03T19:02:36.215127Z","shell.execute_reply.started":"2021-06-03T19:02:36.211516Z","shell.execute_reply":"2021-06-03T19:02:36.214101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids, tags, attention_mask = pipeline.load_outputs()\n#mention_positions = pipeline.load_mention_pos()\nauxiliary_datasets = ParseUtils.load_tokenized_auxiliary_datasets(data_path, 'auxiliary_datasets_scibert_tokenized.txt')","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:02:36.216197Z","iopub.execute_input":"2021-06-03T19:02:36.21652Z","iopub.status.idle":"2021-06-03T19:02:47.274399Z","shell.execute_reply.started":"2021-06-03T19:02:36.216486Z","shell.execute_reply":"2021-06-03T19:02:47.273481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mention_positions = [[i_word for i_word, tag in enumerate(sentence) if tag != o_id] for i_sentence, sentence in enumerate(tags)]","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:02:47.276455Z","iopub.execute_input":"2021-06-03T19:02:47.276709Z","iopub.status.idle":"2021-06-03T19:02:50.646115Z","shell.execute_reply.started":"2021-06-03T19:02:47.276684Z","shell.execute_reply":"2021-06-03T19:02:50.6452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split multiple mentions in sentence\nfor i, sentence in enumerate(mention_positions):\n    if len(sentence) > 0:\n        indices = []\n        previous = sentence[0]\n        for sent_i, item in enumerate(sentence[1:]):\n            if item != previous + 1:\n                indices.append(sent_i+1)\n            previous = item\n        \n        indices.append(len(sentence))\n        \n        new_list = []\n        prev_index = 0\n        for index in indices:\n            new_list.append(sentence[prev_index:index])\n            prev_index = index\n            \n        mention_positions[i] = new_list","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:02:50.647434Z","iopub.execute_input":"2021-06-03T19:02:50.647769Z","iopub.status.idle":"2021-06-03T19:02:50.861283Z","shell.execute_reply.started":"2021-06-03T19:02:50.647727Z","shell.execute_reply":"2021-06-03T19:02:50.860485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tag_values=np.unique(tags)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:02:50.86245Z","iopub.execute_input":"2021-06-03T19:02:50.862943Z","iopub.status.idle":"2021-06-03T19:02:56.916231Z","shell.execute_reply.started":"2021-06-03T19:02:50.862904Z","shell.execute_reply":"2021-06-03T19:02:56.915351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tag2id = {t:i for i, t in enumerate(np.unique(tags))}\ntags = [[tag2id[tag] for tag in sent] for sent in tags]","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:02:56.917484Z","iopub.execute_input":"2021-06-03T19:02:56.917867Z","iopub.status.idle":"2021-06-03T19:03:16.838779Z","shell.execute_reply.started":"2021-06-03T19:02:56.917822Z","shell.execute_reply":"2021-06-03T19:03:16.837909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train-validation split\nval_size = 0.1\npermutation = np.random.RandomState(seed=2018).permutation(len(input_ids))\nval_ind = math.floor(val_size * len(input_ids))\n\ntr_inputs, tr_tags, tr_masks, tr_mention_pos = [input_ids[idx] for idx in permutation[val_ind:]], [tags[idx] for idx in permutation[val_ind:]], [attention_mask[idx] for idx in permutation[val_ind:]], [mention_positions[idx] for idx in permutation[val_ind:]]\nval_inputs, val_tags, val_masks = [input_ids[idx] for idx in permutation[:val_ind]], [tags[idx] for idx in permutation[:val_ind]], [attention_mask[idx] for idx in permutation[:val_ind]],","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:03:16.840069Z","iopub.execute_input":"2021-06-03T19:03:16.840406Z","iopub.status.idle":"2021-06-03T19:03:17.55071Z","shell.execute_reply.started":"2021-06-03T19:03:16.84037Z","shell.execute_reply":"2021-06-03T19:03:17.549884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampler_weights = [0.8 if sample ==1 or sample ==2 else 0.2 for sample in tr_tags]","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:03:17.552019Z","iopub.execute_input":"2021-06-03T19:03:17.552368Z","iopub.status.idle":"2021-06-03T19:03:17.640812Z","shell.execute_reply.started":"2021-06-03T19:03:17.552331Z","shell.execute_reply":"2021-06-03T19:03:17.640039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert len(sampler_weights) == len(tr_tags)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:03:17.642023Z","iopub.execute_input":"2021-06-03T19:03:17.642523Z","iopub.status.idle":"2021-06-03T19:03:17.64657Z","shell.execute_reply.started":"2021-06-03T19:03:17.642483Z","shell.execute_reply":"2021-06-03T19:03:17.645746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tr_inputs, tr_tags, tr_masks, tr_mention_pos  = input_ids, tags, attention_mask, mention_positions","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:03:17.647731Z","iopub.execute_input":"2021-06-03T19:03:17.648286Z","iopub.status.idle":"2021-06-03T19:03:17.656742Z","shell.execute_reply.started":"2021-06-03T19:03:17.648249Z","shell.execute_reply":"2021-06-03T19:03:17.655983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_inputs = torch.tensor(tr_inputs).to(device)\nval_inputs = torch.tensor(val_inputs).to(device)\ntr_tags = torch.tensor(tr_tags).to(device)\nval_tags = torch.tensor(val_tags).to(device)\ntr_masks = torch.tensor(tr_masks).to(device)\nval_masks = torch.tensor(val_masks).to(device)\n\nauxiliary_datasets = [torch.tensor(dataset).to(device) for dataset in auxiliary_datasets]","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:03:17.746811Z","iopub.execute_input":"2021-06-03T19:03:17.747114Z","iopub.status.idle":"2021-06-03T19:03:33.292973Z","shell.execute_reply.started":"2021-06-03T19:03:17.747086Z","shell.execute_reply":"2021-06-03T19:03:33.29208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTDataset(torch.utils.data.Dataset):\n    def __init__(self, input_ids, tags, attention_mask, mention_positions, auxiliary_datasets, transform=None):\n\n        self.input_ids = input_ids\n        self.tags = tags\n        self.attention_mask = attention_mask\n        self.mention_positions = mention_positions\n        self.auxiliary_datasets = auxiliary_datasets\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        sample =  self.input_ids[idx], self.tags[idx], self.attention_mask[idx]\n        \n        if self.transform:\n            sample = self.transform(sample, self.mention_positions[idx], self.auxiliary_datasets)\n\n        return sample","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:03:33.294213Z","iopub.execute_input":"2021-06-03T19:03:33.294537Z","iopub.status.idle":"2021-06-03T19:03:33.300935Z","shell.execute_reply.started":"2021-06-03T19:03:33.294504Z","shell.execute_reply":"2021-06-03T19:03:33.300183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MentionSwapAugment(object):\n\n    def __init__(self, chance, o_id, i_id):\n        self.chance = chance\n        self.o_id = o_id\n        self.i_id = i_id\n\n    def __call__(self, sample, mention_positions, auxiliary_datasets):\n        input_ids, tags, attention_mask = sample\n        if random.random() <= self.chance and len(mention_positions) > 0:\n            for ment_pos in mention_positions:\n                if len(ment_pos) > 1:\n                    new_dataset = auxiliary_datasets[random.randrange(0, len(auxiliary_datasets))]\n\n\n                    new_dataset_len = len(new_dataset)\n                    if len(new_dataset) == len(ment_pos):\n                        input_ids[ment_pos] = new_dataset\n                    else:\n                        length_diff = new_dataset_len - len(ment_pos)\n#                         print(tokenizer.convert_ids_to_tokens(input_ids))\n#                         print(tags)\n#                         print(attention_mask)\n#                         print(ment_pos)\n                    \n                        if length_diff < 0:\n                            return self.augment_with_smaller_label(input_ids, tags, attention_mask, new_dataset, ment_pos, length_diff, new_dataset_len)\n                        else:\n                            if ment_pos[0] + new_dataset_len <= len(input_ids):\n                                return self.augment_with_bigger_label(input_ids, tags, attention_mask, new_dataset, ment_pos, length_diff, new_dataset_len)\n\n                    \n        return input_ids, tags, attention_mask\n    \n    \n    def augment_with_smaller_label(self, input_ids, tags, attention_mask, new_dataset, mention_positions, length_diff, new_dataset_len):\n        # Set new tokens\n        new_token_positions = mention_positions[:new_dataset_len]\n        input_ids[new_token_positions] = new_dataset\n\n        # Roll back rest of sentence and tags\n        input_ids[new_token_positions[-1]+1:length_diff] = input_ids[new_token_positions[-1] + 1 - length_diff:].clone()\n        tags[new_token_positions[-1]+1:length_diff] = tags[new_token_positions[-1] + 1 - length_diff:].clone()\n        attention_mask[new_token_positions[-1]+1:length_diff] = attention_mask[new_token_positions[-1] + 1 - length_diff:].clone()\n\n        # Fill ends (padding and O token)\n        input_ids[length_diff:] = 0\n        tags[length_diff:] = self.o_id\n        attention_mask[length_diff:] = 0\n\n#         print(tokenizer.convert_ids_to_tokens(input_ids))\n#         print(tags)\n#         print(attention_mask)\n\n#         print(\"Smaller tag filled in\")\n#         raise Exception(\"FINISH\")\n        \n        return input_ids, tags, attention_mask\n    \n    \n    def augment_with_bigger_label(self, input_ids, tags, attention_mask, new_dataset, mention_positions, length_diff, new_dataset_len):\n        # New token positions\n        new_token_positions = torch.arange(mention_positions[0], mention_positions[0]+new_dataset_len)\n\n\n        # Roll sentence away\n        input_ids[new_token_positions[-1]+1:] = input_ids[mention_positions[-1]+1:-length_diff].clone()\n        tags[new_token_positions[-1]+1:] = tags[mention_positions[-1]+1:-length_diff].clone()\n        attention_mask[new_token_positions[-1]+1:] = attention_mask[mention_positions[-1]+1:-length_diff].clone()\n\n        # Set new tokens\n        input_ids[new_token_positions] = new_dataset\n        \n        # Continue with I token!\n        tags[mention_positions[-1]+1:mention_positions[-1]+1+length_diff] = self.i_id\n        attention_mask[mention_positions[-1]+1:mention_positions[-1]+1+length_diff] = 1\n\n#         print(tokenizer.convert_ids_to_tokens(input_ids))\n#         print(tags)\n#         print(attention_mask)\n\n#         print(\"Bigger tag filled in\")\n#         raise Exception(\"FINISH\")\n        \n        return input_ids, tags, attention_mask\n        ","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:03:33.304839Z","iopub.execute_input":"2021-06-03T19:03:33.305347Z","iopub.status.idle":"2021-06-03T19:03:33.344737Z","shell.execute_reply.started":"2021-06-03T19:03:33.305299Z","shell.execute_reply":"2021-06-03T19:03:33.343882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_size = len(sampler_weights)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:03:33.346316Z","iopub.execute_input":"2021-06-03T19:03:33.346744Z","iopub.status.idle":"2021-06-03T19:03:33.357999Z","shell.execute_reply.started":"2021-06-03T19:03:33.346708Z","shell.execute_reply":"2021-06-03T19:03:33.357207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weighted_sampler = WeightedRandomSampler(sampler_weights,input_size,replacement= True)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:03:33.359044Z","iopub.execute_input":"2021-06-03T19:03:33.359626Z","iopub.status.idle":"2021-06-03T19:03:33.371339Z","shell.execute_reply.started":"2021-06-03T19:03:33.359586Z","shell.execute_reply":"2021-06-03T19:03:33.370533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 128\n\n#train_dataset = BERTDataset(tr_inputs, tr_tags, tr_masks, tr_mention_pos, auxiliary_datasets, transform=MentionSwapAugment(0.5, o_id=tag2id[o_id], i_id=tag2id[i_id]))\ntrain_dataset = TensorDataset(tr_inputs,tr_tags,tr_masks)\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,sampler= weighted_sampler)\n\nvalid_data = TensorDataset(val_inputs, val_masks, val_tags)\nvalid_sampler = SequentialSampler(valid_data)\nvalid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:03:33.372404Z","iopub.execute_input":"2021-06-03T19:03:33.372743Z","iopub.status.idle":"2021-06-03T19:03:33.381735Z","shell.execute_reply.started":"2021-06-03T19:03:33.372706Z","shell.execute_reply":"2021-06-03T19:03:33.380898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Better Evaluation Metric\nThe following code cell implements an improved F1 score for NER evaluation.\n\nFor more details, please see http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/\n\nThe code is taken from https://raw.githubusercontent.com/davidsbatista/NER-Evaluation/master/ner_evaluation/ner_eval.py","metadata":{}},{"cell_type":"code","source":"import logging\nfrom collections import namedtuple\nfrom copy import deepcopy\n\nlogging.basicConfig(\n    format=\"%(asctime)s %(name)s %(levelname)s: %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=\"DEBUG\",\n)\n\nEntity = namedtuple(\"Entity\", \"e_type start_offset end_offset\")\n\nclass Evaluator():\n\n    def __init__(self, true, pred, tags):\n        \"\"\"\n        \"\"\"\n\n        if len(true) != len(pred):\n            raise ValueError(\"Number of predicted documents does not equal true\")\n\n        self.true = true\n        self.pred = pred\n        self.tags = tags\n\n        # Setup dict into which metrics will be stored.\n\n        self.metrics_results = {\n            'correct': 0,\n            'incorrect': 0,\n            'partial': 0,\n            'missed': 0,\n            'spurious': 0,\n            'possible': 0,\n            'actual': 0,\n            'precision': 0,\n            'recall': 0,\n        }\n\n        # Copy results dict to cover the four schemes.\n\n        self.results = {\n            'strict': deepcopy(self.metrics_results),\n            'ent_type': deepcopy(self.metrics_results),\n            'partial':deepcopy(self.metrics_results),\n            'exact':deepcopy(self.metrics_results),\n            }\n\n        # Create an accumulator to store results\n\n        self.evaluation_agg_entities_type = {e: deepcopy(self.results) for e in tags}\n\n\n    def evaluate(self):\n\n        logging.info(\n            \"Imported %s predictions for %s true examples\",\n            len(self.pred), len(self.true)\n        )\n\n        for true_ents, pred_ents in zip(self.true, self.pred):\n\n            # Check that the length of the true and predicted examples are the\n            # same. This must be checked here, because another error may not\n            # be thrown if the lengths do not match.\n\n            if len(true_ents) != len(pred_ents):\n                raise ValueError(\"Prediction length does not match true example length\")\n\n            # Compute results for one message\n\n            tmp_results, tmp_agg_results = compute_metrics(\n                collect_named_entities(true_ents),\n                collect_named_entities(pred_ents),\n                self.tags\n            )\n\n            # Cycle through each result and accumulate\n\n            # TODO: Combine these loops below:\n\n            for eval_schema in self.results:\n\n                for metric in self.results[eval_schema]:\n\n                    self.results[eval_schema][metric] += tmp_results[eval_schema][metric]\n\n            # Calculate global precision and recall\n\n            self.results = compute_precision_recall_wrapper(self.results)\n\n            # Aggregate results by entity type\n\n            for e_type in self.tags:\n\n                for eval_schema in tmp_agg_results[e_type]:\n\n                    for metric in tmp_agg_results[e_type][eval_schema]:\n\n                        self.evaluation_agg_entities_type[e_type][eval_schema][metric] += tmp_agg_results[e_type][eval_schema][metric]\n\n                # Calculate precision recall at the individual entity level\n\n                self.evaluation_agg_entities_type[e_type] = compute_precision_recall_wrapper(self.evaluation_agg_entities_type[e_type])\n\n        return self.results, self.evaluation_agg_entities_type\n\n\ndef collect_named_entities(tokens):\n    \"\"\"\n    Creates a list of Entity named-tuples, storing the entity type and the start and end\n    offsets of the entity.\n\n    :param tokens: a list of tags\n    :return: a list of Entity named-tuples\n    \"\"\"\n\n    named_entities = []\n    start_offset = None\n    end_offset = None\n    ent_type = None\n\n    for offset, token_tag in enumerate(tokens):\n\n        if token_tag == 'O':\n            if ent_type is not None and start_offset is not None:\n                end_offset = offset - 1\n                named_entities.append(Entity(ent_type, start_offset, end_offset))\n                start_offset = None\n                end_offset = None\n                ent_type = None\n\n        elif ent_type is None:\n            ent_type = token_tag[2:]\n            start_offset = offset\n\n        elif ent_type != token_tag[2:] or (ent_type == token_tag[2:] and token_tag[:1] == 'B'):\n\n            end_offset = offset - 1\n            named_entities.append(Entity(ent_type, start_offset, end_offset))\n\n            # start of a new entity\n            ent_type = token_tag[2:]\n            start_offset = offset\n            end_offset = None\n\n    # catches an entity that goes up until the last token\n\n    if ent_type is not None and start_offset is not None and end_offset is None:\n        named_entities.append(Entity(ent_type, start_offset, len(tokens)-1))\n\n    return named_entities\n\n\ndef compute_metrics(true_named_entities, pred_named_entities, tags):\n\n\n    eval_metrics = {'correct': 0, 'incorrect': 0, 'partial': 0, 'missed': 0, 'spurious': 0, 'precision': 0, 'recall': 0}\n\n    # overall results\n    \n    evaluation = {\n        'strict': deepcopy(eval_metrics),\n        'ent_type': deepcopy(eval_metrics),\n        'partial': deepcopy(eval_metrics),\n        'exact': deepcopy(eval_metrics)\n    }\n\n    # results by entity type\n\n    evaluation_agg_entities_type = {e: deepcopy(evaluation) for e in tags}\n\n    # keep track of entities that overlapped\n\n    true_which_overlapped_with_pred = []\n\n    # Subset into only the tags that we are interested in.\n    # NOTE: we remove the tags we don't want from both the predicted and the\n    # true entities. This covers the two cases where mismatches can occur:\n    #\n    # 1) Where the model predicts a tag that is not present in the true data\n    # 2) Where there is a tag in the true data that the model is not capable of\n    # predicting.\n\n    true_named_entities = [ent for ent in true_named_entities if ent.e_type in tags]\n    pred_named_entities = [ent for ent in pred_named_entities if ent.e_type in tags]\n\n    # go through each predicted named-entity\n\n    for pred in pred_named_entities:\n        found_overlap = False\n\n        # Check each of the potential scenarios in turn. See\n        # http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/\n        # for scenario explanation.\n\n        # Scenario I: Exact match between true and pred\n\n        if pred in true_named_entities:\n            true_which_overlapped_with_pred.append(pred)\n            evaluation['strict']['correct'] += 1\n            evaluation['ent_type']['correct'] += 1\n            evaluation['exact']['correct'] += 1\n            evaluation['partial']['correct'] += 1\n\n            # for the agg. by e_type results\n            evaluation_agg_entities_type[pred.e_type]['strict']['correct'] += 1\n            evaluation_agg_entities_type[pred.e_type]['ent_type']['correct'] += 1\n            evaluation_agg_entities_type[pred.e_type]['exact']['correct'] += 1\n            evaluation_agg_entities_type[pred.e_type]['partial']['correct'] += 1\n\n        else:\n\n            # check for overlaps with any of the true entities\n\n            for true in true_named_entities:\n\n                pred_range = range(pred.start_offset, pred.end_offset)\n                true_range = range(true.start_offset, true.end_offset)\n\n                # Scenario IV: Offsets match, but entity type is wrong\n\n                if true.start_offset == pred.start_offset and pred.end_offset == true.end_offset \\\n                        and true.e_type != pred.e_type:\n\n                    # overall results\n                    evaluation['strict']['incorrect'] += 1\n                    evaluation['ent_type']['incorrect'] += 1\n                    evaluation['partial']['correct'] += 1\n                    evaluation['exact']['correct'] += 1\n\n                    # aggregated by entity type results\n                    evaluation_agg_entities_type[true.e_type]['strict']['incorrect'] += 1\n                    evaluation_agg_entities_type[true.e_type]['ent_type']['incorrect'] += 1\n                    evaluation_agg_entities_type[true.e_type]['partial']['correct'] += 1\n                    evaluation_agg_entities_type[true.e_type]['exact']['correct'] += 1\n\n                    true_which_overlapped_with_pred.append(true)\n                    found_overlap = True\n\n                    break\n\n                # check for an overlap i.e. not exact boundary match, with true entities\n\n                elif find_overlap(true_range, pred_range):\n\n                    true_which_overlapped_with_pred.append(true)\n\n                    # Scenario V: There is an overlap (but offsets do not match\n                    # exactly), and the entity type is the same.\n                    # 2.1 overlaps with the same entity type\n\n                    if pred.e_type == true.e_type:\n\n                        # overall results\n                        evaluation['strict']['incorrect'] += 1\n                        evaluation['ent_type']['correct'] += 1\n                        evaluation['partial']['partial'] += 1\n                        evaluation['exact']['incorrect'] += 1\n\n                        # aggregated by entity type results\n                        evaluation_agg_entities_type[true.e_type]['strict']['incorrect'] += 1\n                        evaluation_agg_entities_type[true.e_type]['ent_type']['correct'] += 1\n                        evaluation_agg_entities_type[true.e_type]['partial']['partial'] += 1\n                        evaluation_agg_entities_type[true.e_type]['exact']['incorrect'] += 1\n\n                        found_overlap = True\n\n                        break\n\n                    # Scenario VI: Entities overlap, but the entity type is\n                    # different.\n\n                    else:\n                        # overall results\n                        evaluation['strict']['incorrect'] += 1\n                        evaluation['ent_type']['incorrect'] += 1\n                        evaluation['partial']['partial'] += 1\n                        evaluation['exact']['incorrect'] += 1\n\n                        # aggregated by entity type results\n                        # Results against the true entity\n\n                        evaluation_agg_entities_type[true.e_type]['strict']['incorrect'] += 1\n                        evaluation_agg_entities_type[true.e_type]['partial']['partial'] += 1\n                        evaluation_agg_entities_type[true.e_type]['ent_type']['incorrect'] += 1\n                        evaluation_agg_entities_type[true.e_type]['exact']['incorrect'] += 1\n\n                        # Results against the predicted entity\n\n                        # evaluation_agg_entities_type[pred.e_type]['strict']['spurious'] += 1\n\n                        found_overlap = True\n\n                        break\n\n            # Scenario II: Entities are spurious (i.e., over-generated).\n\n            if not found_overlap:\n\n                # Overall results\n\n                evaluation['strict']['spurious'] += 1\n                evaluation['ent_type']['spurious'] += 1\n                evaluation['partial']['spurious'] += 1\n                evaluation['exact']['spurious'] += 1\n\n                # Aggregated by entity type results\n\n                # NOTE: when pred.e_type is not found in tags\n                # or when it simply does not appear in the test set, then it is\n                # spurious, but it is not clear where to assign it at the tag\n                # level. In this case, it is applied to all target_tags\n                # found in this example. This will mean that the sum of the\n                # evaluation_agg_entities will not equal evaluation.\n\n                for true in tags:                    \n\n                    evaluation_agg_entities_type[true]['strict']['spurious'] += 1\n                    evaluation_agg_entities_type[true]['ent_type']['spurious'] += 1\n                    evaluation_agg_entities_type[true]['partial']['spurious'] += 1\n                    evaluation_agg_entities_type[true]['exact']['spurious'] += 1\n\n    # Scenario III: Entity was missed entirely.\n\n    for true in true_named_entities:\n        if true in true_which_overlapped_with_pred:\n            continue\n        else:\n            # overall results\n            evaluation['strict']['missed'] += 1\n            evaluation['ent_type']['missed'] += 1\n            evaluation['partial']['missed'] += 1\n            evaluation['exact']['missed'] += 1\n\n            # for the agg. by e_type\n            evaluation_agg_entities_type[true.e_type]['strict']['missed'] += 1\n            evaluation_agg_entities_type[true.e_type]['ent_type']['missed'] += 1\n            evaluation_agg_entities_type[true.e_type]['partial']['missed'] += 1\n            evaluation_agg_entities_type[true.e_type]['exact']['missed'] += 1\n\n    # Compute 'possible', 'actual' according to SemEval-2013 Task 9.1 on the\n    # overall results, and use these to calculate precision and recall.\n\n    for eval_type in evaluation:\n        evaluation[eval_type] = compute_actual_possible(evaluation[eval_type])\n\n    # Compute 'possible', 'actual', and precision and recall on entity level\n    # results. Start by cycling through the accumulated results.\n\n    for entity_type, entity_level in evaluation_agg_entities_type.items():\n\n        # Cycle through the evaluation types for each dict containing entity\n        # level results.\n\n        for eval_type in entity_level:\n\n            evaluation_agg_entities_type[entity_type][eval_type] = compute_actual_possible(\n                entity_level[eval_type]\n            )\n\n    return evaluation, evaluation_agg_entities_type\n\n\ndef find_overlap(true_range, pred_range):\n    \"\"\"Find the overlap between two ranges\n\n    Find the overlap between two ranges. Return the overlapping values if\n    present, else return an empty set().\n\n    Examples:\n\n    >>> find_overlap((1, 2), (2, 3))\n    2\n    >>> find_overlap((1, 2), (3, 4))\n    set()\n    \"\"\"\n\n    true_set = set(true_range)\n    pred_set = set(pred_range)\n\n    overlaps = true_set.intersection(pred_set)\n\n    return overlaps\n\n\ndef compute_actual_possible(results):\n    \"\"\"\n    Takes a result dict that has been output by compute metrics.\n    Returns the results dict with actual, possible populated.\n\n    When the results dicts is from partial or ent_type metrics, then\n    partial_or_type=True to ensure the right calculation is used for\n    calculating precision and recall.\n    \"\"\"\n\n    correct = results['correct']\n    incorrect = results['incorrect']\n    partial = results['partial']\n    missed = results['missed']\n    spurious = results['spurious']\n\n    # Possible: number annotations in the gold-standard which contribute to the\n    # final score\n\n    possible = correct + incorrect + partial + missed\n\n    # Actual: number of annotations produced by the NER system\n\n    actual = correct + incorrect + partial + spurious\n\n    results[\"actual\"] = actual\n    results[\"possible\"] = possible\n\n    return results\n\n\ndef compute_precision_recall(results, partial_or_type=False):\n    \"\"\"\n    Takes a result dict that has been output by compute metrics.\n    Returns the results dict with precison and recall populated.\n\n    When the results dicts is from partial or ent_type metrics, then\n    partial_or_type=True to ensure the right calculation is used for\n    calculating precision and recall.\n    \"\"\"\n\n    actual = results[\"actual\"]\n    possible = results[\"possible\"]\n    partial = results['partial']\n    correct = results['correct']\n\n    if partial_or_type:\n        precision = (correct + 0.5 * partial) / actual if actual > 0 else 0\n        recall = (correct + 0.5 * partial) / possible if possible > 0 else 0\n\n    else:\n        precision = correct / actual if actual > 0 else 0\n        recall = correct / possible if possible > 0 else 0\n\n    results[\"precision\"] = precision\n    results[\"recall\"] = recall\n\n    return results\n\n\ndef compute_precision_recall_wrapper(results):\n    \"\"\"\n    Wraps the compute_precision_recall function and runs on a dict of results\n    \"\"\"\n\n    results_a = {key: compute_precision_recall(value, True) for key, value in results.items() if\n                 key in ['partial', 'ent_type']}\n    results_b = {key: compute_precision_recall(value) for key, value in results.items() if\n                 key in ['strict', 'exact']}\n\n    results = {**results_a, **results_b}\n\n    return results\n\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-03T19:03:33.383008Z","iopub.execute_input":"2021-06-03T19:03:33.383388Z","iopub.status.idle":"2021-06-03T19:03:33.429037Z","shell.execute_reply.started":"2021-06-03T19:03:33.383355Z","shell.execute_reply":"2021-06-03T19:03:33.428233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining Model","metadata":{}},{"cell_type":"code","source":"model_path = '../input/pretrained-language-models/SciBERT_scivocab_cased'\n#finetuned_path = '../input/show-us-the-data-bert-weights/SciBERT_Finetuned_5Eps'","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:03:33.430256Z","iopub.execute_input":"2021-06-03T19:03:33.430628Z","iopub.status.idle":"2021-06-03T19:03:33.44424Z","shell.execute_reply.started":"2021-06-03T19:03:33.430594Z","shell.execute_reply":"2021-06-03T19:03:33.443387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(\n    model_path,\n    num_labels=3,\n    output_attentions = False,\n    output_hidden_states = False\n).to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:03:33.445458Z","iopub.execute_input":"2021-06-03T19:03:33.44602Z","iopub.status.idle":"2021-06-03T19:03:44.273886Z","shell.execute_reply.started":"2021-06-03T19:03:33.445984Z","shell.execute_reply":"2021-06-03T19:03:44.273051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Full finetuning to tune all model parameters\n# Otherwise, only train classifier\nFULL_FINETUNING = True\n\nif FULL_FINETUNING:\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'gamma', 'beta']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n         'weight_decay_rate': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n         'weight_decay_rate': 0.0}\n    ]\nelse:\n    param_optimizer = list(model.classifier.named_parameters())\n    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:03:44.275143Z","iopub.execute_input":"2021-06-03T19:03:44.275463Z","iopub.status.idle":"2021-06-03T19:03:44.284261Z","shell.execute_reply.started":"2021-06-03T19:03:44.275427Z","shell.execute_reply":"2021-06-03T19:03:44.283096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(\n    optimizer_grouped_parameters,\n    lr=3e-5,\n    eps=1e-8\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:03:44.285694Z","iopub.execute_input":"2021-06-03T19:03:44.286243Z","iopub.status.idle":"2021-06-03T19:03:44.294602Z","shell.execute_reply.started":"2021-06-03T19:03:44.286206Z","shell.execute_reply":"2021-06-03T19:03:44.293748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\nepochs = 5\nmax_grad_norm = 1.0\n\n# Total number of training steps is number of batches * number of epochs.\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:03:44.295912Z","iopub.execute_input":"2021-06-03T19:03:44.296314Z","iopub.status.idle":"2021-06-03T19:03:44.303731Z","shell.execute_reply.started":"2021-06-03T19:03:44.296279Z","shell.execute_reply":"2021-06-03T19:03:44.302937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for param in model.base_model.parameters():\n    param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:03:44.304887Z","iopub.execute_input":"2021-06-03T19:03:44.305468Z","iopub.status.idle":"2021-06-03T19:03:44.31434Z","shell.execute_reply.started":"2021-06-03T19:03:44.305431Z","shell.execute_reply":"2021-06-03T19:03:44.313561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Put the attention mask on CPU\nval_masks = val_masks.cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:03:44.315606Z","iopub.execute_input":"2021-06-03T19:03:44.315948Z","iopub.status.idle":"2021-06-03T19:03:44.333591Z","shell.execute_reply.started":"2021-06-03T19:03:44.315915Z","shell.execute_reply":"2021-06-03T19:03:44.332884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_values, validation_loss_values = [], []\nn_train_steps = len(train_dataloader)\nn_valid_steps = len(valid_dataloader)\n\nfor epoch_idx in range(epochs):\n    # ========================================\n    #               Training\n    # ========================================\n    # Perform one full pass over the training set.\n\n    print(f'Epoch {epoch_idx+1}/{epochs}')\n    \n    # Put the model into training mode.\n    model.train()\n    # Reset the total loss for this epoch.\n    total_loss = 0\n    \n    # Training loop\n    pbar = Progbar(n_train_steps)\n    for step, batch in enumerate(train_dataloader):\n        #print(f'Train step {step}/{n_train_steps}\\r', end='')\n        \n        # add batch to gpu\n        #batch = tuple(t.to('cpu') for t in batch)\n        b_input_ids, b_labels, b_input_mask  = batch\n        # Always clear any previously calculated gradients before performing a backward pass.\n        model.zero_grad()\n        # forward pass\n        # This will return the loss (rather than the model output)\n        # because we have provided the `labels`.\n        outputs = model(b_input_ids,\n                        attention_mask=b_input_mask, \n                        labels=b_labels)\n        # get the loss\n        loss = outputs.loss\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n        # track train loss\n        total_loss += loss.item()\n        # Clip the norm of the gradient\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n        # update parameters\n        optimizer.step()\n        # Update the learning rate.\n        scheduler.step()\n        pbar.update(step+1), [('Train loss', loss.item())]\n  \n    # Calculate the average loss over the training data.\n    avg_train_loss = total_loss / len(train_dataloader)\n    #print(\"Train Loss: {:.8f}\".format(avg_train_loss))\n\n    # Store the loss value for plotting the learning curve.\n    loss_values.append(avg_train_loss)\n\n\n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n\n    # Put the model into evaluation mode\n    model.eval()\n    # Reset the validation loss for this epoch.\n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    predictions , true_labels = [], []\n    \n    pbar = Progbar(n_valid_steps)\n    for step, batch in enumerate(valid_dataloader):\n\n        #print(f'Valid step {step}/{n_valid_steps}\\r', end='')\n\n        #batch = tuple(t.to('cpu') for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n\n        # Telling the model not to compute or store gradients,\n        # saving memory and speeding up validation\n        with torch.no_grad():\n            # Forward pass, calculate logit predictions.\n            # This will return the logits rather than the loss because we have not provided labels.\n            outputs = model(b_input_ids,\n                            attention_mask=b_input_mask, \n                            labels=b_labels)\n        # Move logits and labels to CPU\n        logits = outputs[1].detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # Calculate the accuracy for this batch of test sentences.\n        loss_val = outputs[0].mean().item()\n        eval_loss += loss_val\n        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n        true_labels.extend(label_ids)\n\n        pbar.update(step+1, [('Valid loss', loss_val)])\n       \n    eval_loss = eval_loss / len(valid_dataloader)\n    validation_loss_values.append(eval_loss)\n    #print(\"Valid Loss: {:.8f}\".format(eval_loss))\n    \n    # Remove padding from predictions\n    masked_preds = []\n    for i in range(len(predictions)):\n        masked_preds.append(predictions[i][:len(np.argwhere(val_masks[0]))])\n\n    # Remove padding from labels\n    masked_labels = []\n    for i in range(len(true_labels)):\n        masked_labels.append(true_labels[i][:len(np.argwhere(val_masks[0]))])\n    \n    # Flatten both\n    flat_masked_preds = np.reshape(masked_preds, -1)\n    flat_masked_labels = np.reshape(masked_labels, -1)\n    \n    # Compute accuracy\n    acc = accuracy_score(flat_masked_preds, flat_masked_labels)\n    print(\"Valid Accuracy: {:2.2f}%\".format(100*acc))\n    \n    # Compute ordinary F1\n    f1 = f1_score(flat_masked_preds, flat_masked_labels, average=None)\n    print(\"Valid F1-Score: {:2.2f}%, {:2.2f}%, {:2.2f}%\".format(*f1*100))\n        \n    # Compute improved F1\n    ids2tag = {0:'B-DATA', 1:'I-DATA', 2:'O'} # Need to convert to String IDs\n    masked_preds_strids = [ [ ids2tag[token] for token in sent ] for sent in masked_preds ]\n    masked_labels_strids = [ [ ids2tag[token] for token in sent ] for sent in masked_labels ]\n    evaluator = Evaluator(masked_labels_strids, masked_preds_strids, ['DATA'])\n    results, _ = evaluator.evaluate() # We don't need the second, aggregated results \n                                      # because we only have one kind of entity\n    # Add ordinary metrics to results\n    results['ordinary'] = { \n        'accuracy': acc, \n        'f1_B': f1[1],\n        'f1_I': f1[0],\n        'f1_O': f1[2]\n    }\n\n    print(\"SemEval Metrics:\")\n    for key in ['strict', 'exact', 'ent_type', 'partial']:\n        precision = results[key]['precision']\n        recall = results[key]['recall']\n        imp_f1 = 2 * ( (precision*recall) / (precision+recall) )\n        print('  '+key+':')\n        print(\"    Precision: {:2.2f}%\".format(precision*100))\n        print(\"    Recall:    {:2.2f}%\".format(recall*100))\n        print(\"    F1-Score:  {:2.2f}%\".format(imp_f1*100))\n        results[key]['f1'] = imp_f1\n    \n    model.save_pretrained(f\"/kaggle/working/SciBERT_Finetuned_(augmentation)_{epoch_idx}Eps\")\n    with open(f'metrics_{epoch_idx}.json', 'w') as f:\n        json.dump(results, f)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:03:44.334738Z","iopub.execute_input":"2021-06-03T19:03:44.335064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}