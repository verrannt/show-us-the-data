{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":" # This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rich","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting rich\n  Downloading rich-10.2.0-py3-none-any.whl (203 kB)\n\u001b[K     |████████████████████████████████| 203 kB 4.5 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from rich) (2.8.1)\nCollecting commonmark<0.10.0,>=0.9.0\n  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n\u001b[K     |████████████████████████████████| 51 kB 4.0 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: colorama<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from rich) (0.4.4)\nRequirement already satisfied: typing-extensions<4.0.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from rich) (3.7.4.3)\nInstalling collected packages: commonmark, rich\nSuccessfully installed commonmark-0.9.1 rich-10.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\nimport json\nimport re\nimport os\nimport pandas as pd\nimport pickle\nfrom tqdm import tqdm,trange\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score,accuracy_score\nimport torch\nfrom transformers import BertForTokenClassification, AdamW, BertTokenizerFast\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n#from rich.console import Console\nfrom rich.progress import track\nfrom tqdm import tqdm\nfrom transformers import BertTokenizerFast","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()","metadata":{"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"test_csv_path = '../input/coleridgeinitiative-show-us-the-data/sample_submission.csv'\ntest_path  = '../input/coleridgeinitiative-show-us-the-data/test'\nmodel_path = '../input/showusdata2'\ntokenizer_path = '../input/huggingface-bert/bert-base-cased'","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class SubmitPred:\n    \n    def __init__(self,test_csv_path,test_path,model_path,tokenizer_path,batch_size=64):\n        self.test_csv_path = test_csv_path\n        self.test_path = test_path\n        self.model = BertForTokenClassification.from_pretrained(model_path,num_labels=3,output_attentions=False,output_hidden_states=False)\n        self.tokenizer = BertTokenizerFast.from_pretrained(tokenizer_path,do_lower_case = False)\n        self.MAX_LENGTH = 64 # max no. words for each sentence.\n        self.OVERLAP = 20 \n        self.tag2str = {2:'O',1:'I',0:'B'}\n        self.batch_size =batch_size\n        \n    def load_submission(self):\n        self.sample_submission = pd.read_csv(self.test_csv_path)\n    \n    def tokenize_sent(self,sentence):\n        tokenized_sentence = []\n        sentence = sentence.split()\n        for word in sentence:\n            tokenized_word = self.tokenizer.tokenize(word)\n            tokenized_sentence.extend(tokenized_word)\n        return tokenized_sentence\n    @staticmethod\n    def shorten_sentences(sentences, max_len, overlap):\n        short_sentences = []\n        for sentence in sentences:\n            words = sentence.split()\n            if len(words) > max_len:\n                for p in range(0, len(words), max_len - overlap):\n                    short_sentences.append(' '.join(words[p:p + max_len]))\n            else:\n                short_sentences.append(sentence)\n        return short_sentences\n    \n    @staticmethod\n    def clean_training_text(txt):\n        return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n    \n    def add_padding(self,tokenized_sentences):\n        padded_sentences = pad_sequences(\n            tokenized_sentences, \n            value='[PAD]', \n            dtype=object, \n            maxlen=self.MAX_LENGTH, \n            truncating='post', \n            padding='post')\n        return padded_sentences\n    @staticmethod\n    def get_attention_mask(input_ids, ignore_tokens=[0,101,102]):\n        return [[float(token not in ignore_tokens) for token in sent ] for sent in input_ids]\n    \n    \n    @staticmethod\n    def jaccard_similarity(s1, s2):\n        l1 = s1.split(\" \")\n        l2 = s2.split(\" \")    \n        intersection = len(list(set(l1).intersection(l2)))\n        union = (len(l1) + len(l2)) - intersection\n        return float(intersection) / union\n    \n    @staticmethod\n    def add_start_end_tokens(tupled_sentence):\n        tupled_sentence.insert(0, ('[CLS]'))\n        tupled_sentence.append(('[SEP]'))\n        return tupled_sentence\n    \n    def run(self):\n        self.load_submission()\n        self.model.cuda()\n        paper_length = []\n        sentences_e =  []\n        papers = {}\n        for paper_id in self.sample_submission['Id']:\n            with open(f'{self.test_path}/{paper_id}.json', 'r') as f:\n                paper = json.load(f)\n                papers[paper_id] = paper\n        for id in self.sample_submission['Id']:\n            paper = papers[id]\n            sentences = set([self.clean_training_text(sentence) for section in paper for sentence in section['text'].split('.')])\n            sentences = self.shorten_sentences(sentences,self.MAX_LENGTH,self.OVERLAP)\n            sentences = [sentence for sentence in sentences if len(sentence) > 10]\n            ner_data = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n            sentences_e.extend(ner_data)\n            paper_length.append(len(ner_data))\n        tokenized_words= [self.tokenize_sent(sentence) for sentence in sentences_e]\n        start_end = [self.add_start_end_tokens(sentence) for sentence in tokenized_words]\n        padding_sentences =  self.add_padding(start_end)\n        input_ids = [self.tokenizer.convert_tokens_to_ids(text) for text in padding_sentences]\n        attention_mask = self.get_attention_mask(input_ids,ignore_tokens=[0,101,102])\n        predicts = torch.tensor(input_ids)\n        masks    = torch.tensor(attention_mask)\n        predict_data = TensorDataset(predicts, masks)\n        predict_dataloader = DataLoader(predict_data, batch_size=self.batch_size)\n        all_predictions = []\n        for step,batch in enumerate(predict_dataloader):\n            batch = tuple(t.to(device) for t in batch)\n            b_input_ids, b_input_mask = batch\n            with torch.no_grad():\n                output = self.model(b_input_ids,attention_mask=b_input_mask)\n            label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n            all_predictions.extend(label_indices)\n        \n        all_preds_str = [[self.tag2str[token]for token in pred] for pred in all_predictions]\n        all_sent_str = [self.tokenizer.convert_ids_to_tokens(sent) for sent in input_ids]\n        all_sent_int = [ids for ids in input_ids]\n        final_predics = []\n        all_sent_str_1 = all_sent_int\n        all_preds_str_1 = all_preds_str\n        for pap_len in paper_length:\n            labels = []\n            for sentence,pred in zip(all_sent_str_1[:pap_len],all_preds_str_1[:pap_len]):\n                phrase = []\n                for word,tag in zip(sentence,pred):\n                    if tag ==\"I\" or tag ==\"B\":\n                        phrase.append(word)\n                    else:\n                        if len(phrase)!=0:\n                            labels.append(self.tokenizer.decode(phrase))\n                            phrase = []\n\n            final_predics.append(labels)\n            del all_sent_str_1[:pap_len], all_preds_str_1[pap_len]\n        self.sample_submission['PredictionString'] = final_predics\n        self.sample_submission['PredictionString'] = self.sample_submission.apply(lambda x:\"|\".join(x.PredictionString),axis=1)\n    \n    def save_csv(self):\n        self.sample_submission.to_csv(f'submission.csv', index=False)","metadata":{"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"sub_pred = SubmitPred(test_csv_path,test_path,model_path,tokenizer_path)","metadata":{"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"sub_pred.run()","metadata":{"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"sub_pred.save_csv()","metadata":{"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"![](http://)","metadata":{}}]}