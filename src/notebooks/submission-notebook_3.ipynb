{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"w # This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rich","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\nimport random\nimport json\nimport re\nimport os\nimport pandas as pd\nimport pickle\nfrom tqdm import tqdm,trange\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score,accuracy_score\nimport torch\nfrom transformers import BertForTokenClassification, AdamW, BertTokenizerFast\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n#from rich.console import Console\n#from rich.progress import track\nfrom tqdm import tqdm\nfrom transformers import BertTokenizerFast","metadata":{"execution":{"iopub.status.busy":"2021-05-20T08:44:03.719049Z","iopub.execute_input":"2021-05-20T08:44:03.719546Z","iopub.status.idle":"2021-05-20T08:44:06.222851Z","shell.execute_reply.started":"2021-05-20T08:44:03.719466Z","shell.execute_reply":"2021-05-20T08:44:06.221993Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T08:44:06.229321Z","iopub.execute_input":"2021-05-20T08:44:06.229659Z","iopub.status.idle":"2021-05-20T08:44:06.254383Z","shell.execute_reply.started":"2021-05-20T08:44:06.229625Z","shell.execute_reply":"2021-05-20T08:44:06.253565Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"test_csv_path = '../input/coleridgeinitiative-show-us-the-data/sample_submission.csv'\ntest_path  = '../input/coleridgeinitiative-show-us-the-data/test'\n#model_path = '../input/showusdata2'\nmodel_path = '../input/5epochweights'\ntokenizer_path = '../input/huggingface-bert/bert-base-cased'","metadata":{"execution":{"iopub.status.busy":"2021-05-20T08:44:10.488517Z","iopub.execute_input":"2021-05-20T08:44:10.488896Z","iopub.status.idle":"2021-05-20T08:44:10.492725Z","shell.execute_reply.started":"2021-05-20T08:44:10.488862Z","shell.execute_reply":"2021-05-20T08:44:10.491768Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class SubmitPred:\n    \n    def __init__(self,test_csv_path,test_path,model_path,tokenizer_path,batch_size=64):\n        self.test_csv_path = test_csv_path\n        self.test_path = test_path\n        self.model = BertForTokenClassification.from_pretrained(model_path,num_labels=3,output_attentions=False,output_hidden_states=False)\n        self.tokenizer = BertTokenizerFast.from_pretrained(tokenizer_path,do_lower_case = False)\n        self.MAX_LENGTH = 64 # max no. words for each sentence.\n        self.OVERLAP = 20 \n        self.tag2str = {2:'O',1:'I',0:'B'}\n        self.batch_size =batch_size\n        \n    def load_submission(self):\n        self.sample_submission = pd.read_csv(self.test_csv_path)\n    \n    def tokenize_sent(self,sentence):\n        tokenized_sentence = []\n        sentence = sentence.split()\n        for word in sentence:\n            tokenized_word = self.tokenizer.tokenize(word)\n            tokenized_sentence.extend(tokenized_word)\n        return tokenized_sentence\n    \n    def read_and_create_csv(self):\n        all_test_papers = os.listdir('../input/coleridgeinitiative-show-us-the-data/test')\n        self.submission = pd.DataFrame({'Id':all_test_papers})\n    \n    @staticmethod\n    def shorten_sentences(sentences, max_len, overlap):\n        short_sentences = []\n        for sentence in sentences:\n            words = sentence.split()\n            if len(words) > max_len:\n                for p in range(0, len(words), max_len - overlap):\n                    short_sentences.append(' '.join(words[p:p + max_len]))\n            else:\n                short_sentences.append(sentence)\n        return short_sentences\n    \n    @staticmethod\n    def clean_training_text(txt):\n        return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n    \n    def add_padding(self,tokenized_sentences):\n        padded_sentences = pad_sequences(\n            tokenized_sentences, \n            value='[PAD]', \n            dtype=object, \n            maxlen=self.MAX_LENGTH, \n            truncating='post', \n            padding='post')\n        return padded_sentences\n    @staticmethod\n    def get_attention_mask(input_ids, ignore_tokens=[0,101,102]):\n        return [[float(token not in ignore_tokens) for token in sent ] for sent in input_ids]\n    \n    \n    @staticmethod\n    def jaccard_similarity(list1, list2):\n        intersection = len(list(set(list1).intersection(list2)))\n        union = (len(list1) + len(list2)) - intersection\n        return float(intersection) / union\n    \n    @staticmethod\n    def add_start_end_tokens(tupled_sentence):\n        tupled_sentence.insert(0, '[CLS]')\n        tupled_sentence.append('[SEP]')\n        return tupled_sentence\n    \n    def run(self):\n        self.load_submission()\n        self.model.cuda()\n        paper_length = []\n        sentences_e =  []\n        papers = {}\n        self.read_and_create_csv()\n        for paper_id in self.submission['Id']:\n            with open(f'{self.test_path}/{paper_id}', 'r') as f:\n                paper = json.load(f)\n                papers[paper_id] = paper\n        for id in self.submission['Id']:\n            paper = papers[id]\n            sentences = set([self.clean_training_text(sentence) for section in paper for sentence in section['text'].split('.')])\n            sentences = self.shorten_sentences(sentences,self.MAX_LENGTH,self.OVERLAP)\n            sentences = [sentence for sentence in sentences if len(sentence) > 10]\n            ner_data = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n            sentences_e.extend(ner_data)\n            print(f\"paper {id} length: {len(ner_data)}\")\n            paper_length.append(len(ner_data))\n        tokenized_words= [self.tokenize_sent(sentence) for sentence in sentences_e]\n        start_end = [self.add_start_end_tokens(sentence) for sentence in tokenized_words]\n        padding_sentences =  self.add_padding(start_end)\n        input_ids = [self.tokenizer.convert_tokens_to_ids(text) for text in padding_sentences]\n        attention_mask = self.get_attention_mask(input_ids,ignore_tokens=[0,101,102])\n        predicts = torch.tensor(input_ids)\n        masks    = torch.tensor(attention_mask)\n        predict_data = TensorDataset(predicts, masks)\n        predict_dataloader = DataLoader(predict_data, batch_size=self.batch_size)\n        all_predictions = []\n        for step,batch in enumerate(predict_dataloader):\n            batch = tuple(t.to(device) for t in batch)\n            b_input_ids, b_input_mask = batch\n            with torch.no_grad():\n                output = self.model(b_input_ids,attention_mask=b_input_mask)\n            label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n            all_predictions.extend(label_indices)\n        \n        all_preds_str = [[self.tag2str[token]for token in pred] for pred in all_predictions]\n        all_sent_str = [self.tokenizer.convert_ids_to_tokens(sent) for sent in input_ids]\n        all_sent_int = [ids for ids in input_ids]\n        final_predics = []\n        all_sent_str_1 = all_sent_int\n        all_preds_str_1 = all_preds_str\n        for pap_len in paper_length:\n            labels = []\n            test_all_labels = []\n            for sentence,pred in zip(all_sent_str_1[:pap_len],all_preds_str_1[:pap_len]):\n                phrase = []\n                phrase_test = []\n                for word,tag in zip(sentence,pred):\n                    \n                    if tag ==\"I\" or tag ==\"B\":\n                        phrase_test.append(word)\n                        if word!= 0 and word!= 101 and word!=102:\n                            phrase.append(word)\n                    else:\n                        if len(phrase)!=0:\n                            labels.append(self.tokenizer.decode(phrase))\n                            phrase_test = []\n                            phrase = []\n\n            final_predics.append(labels)\n            del all_sent_str_1[:pap_len], all_preds_str_1[:pap_len]\n        final_predics =[[pred for pred in preds if not pred.startswith(\"#\")] for preds in final_predics]\n        \n        filtered = []\n        for final_predic in final_predics:\n            filt = []\n            for pred in final_predic:\n                if len(filt) ==0:\n                    filt.append(pred)\n                else:\n                    flag = 0\n                    for filtered_pred in filt:\n                        if self.jaccard_similarity(filtered_pred.split(),pred.split())>0.70:\n\n                            flag = 1\n                        if flag ==0:\n                            filt.append(pred)\n        \n                            \n            filtered.append(filt)\n        \n        self.final_predics = final_predics\n        filtered = [\"|\".join(filt) if len(filt)!=0 else filt for filt in filtered ]\n        self.filtered = filtered\n        self.submission['PredictionString'] = filtered\n        print(\"Predictions Complete\")\n        #self.submission['PredictionString'] = self.submission.apply(lambda x:\"|\".join(x.PredictionString),axis=1)\n    \n    def save_csv(self):\n        self.submission.to_csv(f'submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T08:48:20.240017Z","iopub.execute_input":"2021-05-20T08:48:20.240344Z","iopub.status.idle":"2021-05-20T08:48:20.272139Z","shell.execute_reply.started":"2021-05-20T08:48:20.240313Z","shell.execute_reply":"2021-05-20T08:48:20.270996Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"sub_pred = SubmitPred(test_csv_path,test_path,model_path,tokenizer_path,256)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T08:48:25.749155Z","iopub.execute_input":"2021-05-20T08:48:25.749488Z","iopub.status.idle":"2021-05-20T08:48:29.089409Z","shell.execute_reply.started":"2021-05-20T08:48:25.749454Z","shell.execute_reply":"2021-05-20T08:48:29.088491Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"sub_pred.run()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T08:48:29.651386Z","iopub.execute_input":"2021-05-20T08:48:29.651729Z","iopub.status.idle":"2021-05-20T08:48:30.989238Z","shell.execute_reply.started":"2021-05-20T08:48:29.651692Z","shell.execute_reply":"2021-05-20T08:48:30.988360Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"paper 8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60.json length: 83\npaper 2100032a-7c33-4bff-97ef-690822c43466.json length: 34\npaper 2f392438-e215-4169-bebf-21ac4ff253e1.json length: 131\npaper 3f316b38-1a24-45a9-8d8c-4e05a42257c6.json length: 98\nCurrent Prediction too similar\nCurrent Prediction too similar\nCurrent Prediction too similar\nCurrent Prediction too similar\nCurrent Prediction too similar\n","output_type":"stream"}]},{"cell_type":"code","source":"sub_pred.save_csv()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T08:49:08.094579Z","iopub.execute_input":"2021-05-20T08:49:08.094956Z","iopub.status.idle":"2021-05-20T08:49:08.327435Z","shell.execute_reply.started":"2021-05-20T08:49:08.094926Z","shell.execute_reply":"2021-05-20T08:49:08.326636Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"![](http://)","metadata":{}}]}