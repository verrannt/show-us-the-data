{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "998c70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built in\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "# append to path to allow relative imports\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# 3rd party\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rich.console import Console\n",
    "from rich.progress import track\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForTokenClassification, BertModel, AdamW, BertTokenizerFast\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# own\n",
    "from utils.data.preproc import Pipeline, PipelineConfigs\n",
    "from utils.data.parse import ParseUtils\n",
    "from utils.data.parse import ParseUtils\n",
    "from utils.generic import timer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738af29f",
   "metadata": {},
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d15b7439",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(\n",
    "    os.path.abspath('../../'), # Root of project\n",
    "    'data/coleridgeinitiative-show-us-the-data/' # Data folder\n",
    ")\n",
    "\n",
    "configs = PipelineConfigs(\n",
    "    DATA_PATH = data_path,\n",
    "    MAX_LENGTH = 64,\n",
    "    OVERLAP = 20,\n",
    "    MAX_SAMPLE = None,\n",
    "    SAVE = True,\n",
    "    EXTRACTED_FILENAME = 'train_ner.data',\n",
    "    TOKENIZED_FILENAME = 'train_ner.data.tokenized',\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0: \n",
    "    torch.cuda.get_device_name(0)\n",
    "    print('Running on GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c602f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec9b9627",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, tags, attention_masks = pipeline.load_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00abbd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermezzo fix tag labels\n",
    "tag2id = {t:i for i, t in enumerate(np.unique(tags))}\n",
    "tags = [[tag2id[tag] for tag in sent] for sent in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "181adc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(\n",
    "    input_ids, \n",
    "    tags,\n",
    "    random_state=2018, \n",
    "    test_size=0.1\n",
    ")\n",
    "\n",
    "tr_masks, val_masks, _, _ = train_test_split(\n",
    "    attention_masks, \n",
    "    input_ids,\n",
    "    random_state=2018, \n",
    "    test_size=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c242ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8699f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69b622",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e74651e",
   "metadata": {},
   "source": [
    "# Using `BertModel` to feed embeddings into separate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f04cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bert model. This will output raw hidden states\n",
    "model = BertModel.from_pretrained(\n",
    "    'bert-base-cased',\n",
    "    num_labels = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b80f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one layer linear classifier to be used on top\n",
    "# of hidden states\n",
    "cls = nn.Linear(model.config.hidden_size, model.config.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "282f13dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batched_embeddings(\n",
    "    model,  # The HuggingFace BertModel to create the embeddings\n",
    "    inputs, # The inputs to be fed to the model\n",
    "    masks,  # Attention masks for the inputs (same shape as inputs)\n",
    "    batch_size:int=32,\n",
    "):\n",
    "    \"\"\"\n",
    "    Feed batches of size `batch_size` into the provided model from provided inputs and masks,\n",
    "    and return the resulting embeddings as a `torch.tensor`\n",
    "    \"\"\"\n",
    "    total = inputs.shape[0]\n",
    "    # Drop the last data points if they don't fit with the batch size\n",
    "    total = (total // batch_size) * batch_size\n",
    "    \n",
    "    return [ \n",
    "        model(tr_inputs[i:i+batch_size], attention_mask=tr_masks[i:i+batch_size])\n",
    "            .last_hidden_state\n",
    "            .detach()\n",
    "        \n",
    "            for i in track(range(0, total, batch_size),\n",
    "               description='Creating embeddings')\n",
    "    ]\n",
    "\n",
    "def create_batched_tensor(\n",
    "    tensor,\n",
    "    batch_size:int=32,\n",
    "):\n",
    "    \"\"\"\n",
    "    From any given tensor, split the first dimension into batches of size\n",
    "    `batch_size` and return as a `torch.tensor`\n",
    "    \"\"\"\n",
    "    total = tensor.shape[0] \n",
    "    # Drop the last data points if they don't fit with the batch size\n",
    "    total = (total // batch_size) * batch_size\n",
    "    \n",
    "    return torch.tensor([\n",
    "        tensor[i:i+batch_size].numpy()\n",
    "        for i in track(range(0, total, batch_size),\n",
    "            description='Creating batches')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f6a854",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emb_data = create_batched_embeddings(model, tr_inputs, tr_masks, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f4739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = create_batched_tensor(tr_tags, batch_size=BATCH_SIZE)\n",
    "masks = create_batched_tensor(tr_masks, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c6a93d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([169, 32, 64, 768])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_data.shape, labels.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "4f73df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = model.config.num_labels\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 3e-5\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(cls.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "106d19c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values, accuracies, f1_scores = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "9e8be101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch  50/1000 ]   Loss: 0.07105   Acc: 0.48149   F1: 0.64907\n",
      "[ Epoch 100/1000 ]   Loss: 0.06106   Acc: 0.48149   F1: 0.64908\n",
      "[ Epoch 150/1000 ]   Loss: 0.05734   Acc: 0.48149   F1: 0.64908\n",
      "[ Epoch 200/1000 ]   Loss: 0.05479   Acc: 0.48149   F1: 0.64908\n",
      "[ Epoch 250/1000 ]   Loss: 0.05262   Acc: 0.48149   F1: 0.64908\n",
      "[ Epoch 300/1000 ]   Loss: 0.05064   Acc: 0.48149   F1: 0.64907\n",
      "[ Epoch 350/1000 ]   Loss: 0.04880   Acc: 0.48149   F1: 0.64907\n",
      "[ Epoch 400/1000 ]   Loss: 0.04707   Acc: 0.48149   F1: 0.64907\n",
      "[ Epoch 450/1000 ]   Loss: 0.04543   Acc: 0.48149   F1: 0.64906\n",
      "[ Epoch 500/1000 ]   Loss: 0.04389   Acc: 0.48149   F1: 0.64906\n",
      "[ Epoch 550/1000 ]   Loss: 0.04244   Acc: 0.48149   F1: 0.64905\n",
      "[ Epoch 600/1000 ]   Loss: 0.04107   Acc: 0.48149   F1: 0.64905\n",
      "[ Epoch 650/1000 ]   Loss: 0.03979   Acc: 0.48149   F1: 0.64904\n",
      "[ Epoch 700/1000 ]   Loss: 0.03859   Acc: 0.48148   F1: 0.64902\n",
      "[ Epoch 750/1000 ]   Loss: 0.03747   Acc: 0.48147   F1: 0.64901\n",
      "[ Epoch 800/1000 ]   Loss: 0.03643   Acc: 0.48148   F1: 0.64900\n",
      "[ Epoch 850/1000 ]   Loss: 0.03545   Acc: 0.48149   F1: 0.64899\n",
      "[ Epoch 900/1000 ]   Loss: 0.03454   Acc: 0.48151   F1: 0.64897\n",
      "[ Epoch 950/1000 ]   Loss: 0.03368   Acc: 0.48153   F1: 0.64894\n",
      "[ Epoch 1000/1000 ]   Loss: 0.03289   Acc: 0.48157   F1: 0.64892\n"
     ]
    }
   ],
   "source": [
    "total = emb_data.shape[0]\n",
    "\n",
    "USE_MASK = True\n",
    "\n",
    "for epoch in range(1,EPOCHS+1):\n",
    "    \n",
    "    # Collect metrics for all batches\n",
    "    losses, accs, f1s = [], [], []\n",
    "    \n",
    "    for step, (X, y) in enumerate(zip(emb_data, labels)):\n",
    "                \n",
    "        # Compute prediction\n",
    "        logits = cls(X)\n",
    "        # Flatten over batch and sequence\n",
    "        flat_logits = logits.view(-1, NUM_LABELS)\n",
    "        \n",
    "        # Compute loss\n",
    "        if USE_MASK:\n",
    "            loss_mask = masks[step].view(-1) == 1\n",
    "            flat_y = torch.where(\n",
    "                loss_mask, y.view(-1), torch.tensor(loss_fn.ignore_index).type_as(y)\n",
    "            )\n",
    "            loss = loss_fn(flat_logits, flat_y)\n",
    "        else:\n",
    "            flat_y = y.view(-1)\n",
    "            loss = loss_fn(flat_logits, flat_y) \n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Detach and argmax\n",
    "        flat_logits = np.argmax(flat_logits.detach(), axis=1)\n",
    "        flat_y = flat_y.detach()\n",
    "        \n",
    "        # Compute metrics\n",
    "        losses.append(loss.item())        \n",
    "        accs.append(accuracy_score(flat_logits, flat_y))\n",
    "        f1s.append(f1_score(flat_logits, flat_y, average='weighted'))\n",
    "        \n",
    "    loss = np.mean(losses)\n",
    "    acc = np.mean(accs)\n",
    "    f1 = np.mean(f1s)\n",
    "        \n",
    "    loss_values.append(loss)\n",
    "    accuracies.append(acc)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print('[ Epoch {:3}/{} ]   Loss: {:.5f}   Acc: {:.5f}   F1: {:.5f}'.format(epoch, EPOCHS, loss, acc, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0018be89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.xlabel('Epochs')\n",
    "plt.plot(loss_values, label='Loss')\n",
    "plt.plot(accuracies, label='Accuracy')\n",
    "plt.plot(f1_scores, label='F1 Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c26d5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c0654",
   "metadata": {},
   "source": [
    "# Using `BertForTokenClassification` with possible finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaebf207",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\n",
    "    'bert-base-cased',\n",
    "    num_labels = 3, # Hardcode for now\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97616bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models are initialized in eval mode by default. We can call model.train() to put it in train mode.\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "614d2c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full finetuning to tune all model parameters\n",
    "# Otherwise, only train classifier\n",
    "FULL_FINETUNING = True\n",
    "\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95d18d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "LEARNING_RATE = 3e-5\n",
    "EPSILON = 1e-8\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=LEARNING_RATE,\n",
    "    eps=EPSILON\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ca6d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88ccf07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a5324d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fc626d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "Step 0/15792\n",
      "Step 1/15792\n",
      "Step 2/15792\n",
      "Step 3/15792\n",
      "Step 4/15792\n",
      "Step 5/15792\n",
      "Step 6/15792\n",
      "Step 7/15792\n",
      "Step 8/15792\n",
      "Step 9/15792\n",
      "Step 10/15792\n",
      "Step 11/15792\n",
      "Step 12/15792\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6ba82f25c92d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Update the learning rate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib/python3.8/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    353\u001b[0m                     \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0;31m# Just adding the square of the weights to the loss function is *not*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Store the average loss after each epoch so we can plot them.\n",
    "loss_values, validation_loss_values = [], []\n",
    "n_train_samples = len(train_dataloader)\n",
    "n_val_samples = len(valid_dataloader)\n",
    "\n",
    "for eidx in range(EPOCHS):\n",
    "\n",
    "    print(f'Epoch {eidx}/{EPOCHS}')\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        print(f'Step {step}/{n_train_samples}')\n",
    "\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Always clear any previously calculated gradients before performing a backward pass.\n",
    "        model.zero_grad()\n",
    "        # forward pass\n",
    "        # This will return the loss (rather than the model output)\n",
    "        # because we have provided the `labels`.\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "        # get the loss\n",
    "        loss = outputs[0]\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        total_loss += loss.item()\n",
    "        # Clip the norm of the gradient\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "    # Reset the validation loss for this epoch.\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "\n",
    "        print(f'Step {step}/{n_val_samples}')\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients,\n",
    "        # saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have not provided labels.\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "\n",
    "    eval_loss = eval_loss / len(valid_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
    "    valid_tags = [tag_values[l_i] for l in true_labels\n",
    "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
    "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
    "    print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7857d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(tr_inputs[:32], attention_mask=tr_masks[:32], labels=tr_tags[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90177486",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1dfb3952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3c2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
