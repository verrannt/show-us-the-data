{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "998c70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built in\n",
    "import json\n",
    "import random\n",
    "# append to path to allow relative imports\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# 3rd party\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForTokenClassification\n",
    "import torch\n",
    "\n",
    "# own\n",
    "from utils.data.parse import ParseUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738af29f",
   "metadata": {},
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46025a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 64 # max no. words for each sentence.\n",
    "OVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n",
    "\n",
    "MAX_SAMPLE = 3 # set a small number for experimentation, set None for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/coleridgeinitiative-show-us-the-data/train.csv'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cf9f988c22ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mner_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParseUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_ner_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SAMPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Development/Projects/show-us-the-data/src/utils/data/parse.py\u001b[0m in \u001b[0;36mcompute_ner_data\u001b[0;34m(max_sample, save)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Read data in CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParseUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN_DF_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Found {len(train)} raw training rows'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/coleridgeinitiative-show-us-the-data/train.csv'"
     ]
    }
   ],
   "source": [
    "ner_data = ParseUtils.compute_ner_data(max_sample=MAX_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00aa5f6",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No. raw training rows: 2000\n"
     ]
    }
   ],
   "source": [
    "TRAIN_CSV = '../../data/coleridgeinitiative-show-us-the-data/train.csv'\n",
    "TRAIN_DATA = '../../data/coleridgeinitiative-show-us-the-data/train'\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "train = train[:MAX_SAMPLE]\n",
    "print(f'No. raw training rows: {len(train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "437b74bc",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No. grouped training rows: 1811\n"
     ]
    }
   ],
   "source": [
    "train = train.groupby('Id').agg({\n",
    "    'pub_title': 'first',\n",
    "    'dataset_title': '|'.join,\n",
    "    'dataset_label': '|'.join,\n",
    "    'cleaned_label': '|'.join\n",
    "}).reset_index()\n",
    "\n",
    "print(f'No. grouped training rows: {len(train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14ca2a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = {}\n",
    "for paper_id in train['Id'].unique():\n",
    "    with open(f'{TRAIN_DATA}/{paper_id}.json', 'r') as f:\n",
    "        paper = json.load(f)\n",
    "        papers[paper_id] = paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b655ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data size: 2484 positives + 51683 negatives: 100%|██████████| 1811/1811 [00:15<00:00, 122.15it/s]"
     ]
    }
   ],
   "source": [
    "cnt_pos, cnt_neg = 0, 0 # number of sentences that contain/not contain labels\n",
    "ner_data = []\n",
    "\n",
    "pbar = tqdm(total=len(train))\n",
    "for i, id, dataset_label in train[['Id', 'dataset_label']].itertuples():\n",
    "    # paper\n",
    "    paper = papers[id]\n",
    "    \n",
    "    # labels\n",
    "    labels = dataset_label.split('|')\n",
    "    labels = [ParseUtils.clean_training_text(label) for label in labels]\n",
    "    \n",
    "    # sentences\n",
    "    sentences = set([ParseUtils.clean_training_text(sentence) for section in paper \n",
    "                 for sentence in section['text'].split('.') \n",
    "                ])\n",
    "    sentences = ParseUtils.shorten_sentences(sentences) # make sentences short\n",
    "    # only accept sentences with length > 10 chars\n",
    "    sentences = [sentence for sentence in sentences if len(sentence) > 10] \n",
    "    \n",
    "    # positive sample\n",
    "    for sentence in sentences:\n",
    "        is_positive, tags = ParseUtils.tag_sentence(sentence, labels)\n",
    "        if is_positive:\n",
    "            cnt_pos += 1\n",
    "            ner_data.append(tags)\n",
    "        elif any(word in sentence.lower() for word in ['data', 'study']): \n",
    "            ner_data.append(tags)\n",
    "            cnt_neg += 1\n",
    "    \n",
    "    # process bar\n",
    "    pbar.update(1)\n",
    "    pbar.set_description(f\"Training data size: {cnt_pos} positives + {cnt_neg} negatives\")\n",
    "    \n",
    "    if i==MAX_SAMPLE-1:\n",
    "        break\n",
    "    \n",
    "# shuffling\n",
    "#random.shuffle(ner_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0678f948",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NERData' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-784f8499ea95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Write data to file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/train_ner.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mner_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mrow_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'tokens'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tags'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnes\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NERData' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Write data to file\n",
    "with open('train_ner.json', 'w') as f:\n",
    "    for row in ner_data:\n",
    "        words, nes = list(zip(*row))\n",
    "        row_json = {'tokens' : words, 'tags' : nes}\n",
    "        json.dump(row_json, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e28fb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERData:\n",
    "    #def __init__(self):\n",
    "    #    self.data = list()\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_json_file(filename:str, data):\n",
    "        f = open(filename, 'w')\n",
    "        for row in data:\n",
    "            words, nes = list(zip(*row))\n",
    "            row_json = {'tokens': words, 'tags': nes}\n",
    "            json.dump(row_json, f)\n",
    "            f.write('\\n')\n",
    "        f.close()\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_json_file(filename:str, overwrite:bool=False):\n",
    "        \n",
    "        #if self.data:\n",
    "        #    if overwrite:\n",
    "        #        self.data = list()\n",
    "        #    else:\n",
    "        #        raise ValueError(\n",
    "        #            'Data is present. If you want to overwrite it, '\n",
    "        #            'run this function again with overwrite=True.')        \n",
    "        data = []\n",
    "        f = open(filename, 'r')\n",
    "        \n",
    "        for i,line in enumerate(f.readlines()):\n",
    "            \n",
    "            print('Reading data ... {}\\r'.format(i), end='')\n",
    "            \n",
    "            # Each line is formatted in JSON format, e.g.\n",
    "            # { \"tokens\" : [\"A\", \"short\", \"sentence\"],\n",
    "            #   \"tags\"   : [\"0\", \"0\", \"0\"] }\n",
    "            sentence = json.loads(line)\n",
    "            \n",
    "            # From the tokens and tags, we create a list of \n",
    "            # tuples of the form\n",
    "            # [ (\"A\", \"0\"), (\"short\", \"0\"), (\"sentence\", \"0\")]\n",
    "            sentence_tuple_list = [\n",
    "                (token, tag) for token, tag \n",
    "                in zip(sentence[\"tokens\"],sentence[\"tags\"])\n",
    "            ]\n",
    "            \n",
    "            # Each of these parsed sentences becomes an entry\n",
    "            # in our overall data list\n",
    "            data.append(sentence_tuple_list)\n",
    "            \n",
    "            if i==1000:\n",
    "                break\n",
    "            \n",
    "        f.close()\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_sentences(data):\n",
    "        \"\"\"\n",
    "        Convert each entry in self.data into a single-string sentence,\n",
    "        with words separated by a blank space.\n",
    "        \"\"\"\n",
    "        return [ \" \".join([ tuple_[0] for tuple_ in tupled_sentence ]) \n",
    "                for tupled_sentence in data[:100] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f202c3f-0bd3-47d3-8ca0-1a2ed54eb3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NERData.to_json_file('../../data/train_ner_dummy.json', ner_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "632b1cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data ... 1000\r"
     ]
    }
   ],
   "source": [
    "ner_data = NERData.from_json_file('../../data/train_ner_dummy.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d07c08df-12b7-452d-b735-7a4546b70c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'O'),\n",
       " ('research', 'O'),\n",
       " ('study', 'O'),\n",
       " ('comes', 'O'),\n",
       " ('to', 'O'),\n",
       " ('validate', 'O'),\n",
       " ('and', 'O'),\n",
       " ('quantify', 'O'),\n",
       " ('a', 'O'),\n",
       " ('number', 'O'),\n",
       " ('of', 'O'),\n",
       " ('assumptions', 'O'),\n",
       " ('in', 'O'),\n",
       " ('the', 'O'),\n",
       " ('Lebanese', 'O'),\n",
       " ('context', 'O')]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c4381e0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This research study comes to validate and quantify a number of assumptions in the Lebanese context',\n",
       " 'According to a study completed by UNDP 2016 96 of semi skilled workers in the Agro food industry suffer from basic ICT skills weaknesses 89',\n",
       " 'The response rate for this follow up study was 43',\n",
       " 'This study was completed in an effort to find creative and digital solutions to the high rate of youth unemployment in Lebanon 37 one of the highest rates in the world',\n",
       " 'Thus the generalizability of this study could be hindered',\n",
       " 'Participants chosen for this study were above 18 years old labor law in Lebanon sets the eligibility age for work at 18 years who have completed DOT Lebanon ICT training',\n",
       " 'The subject of this study was one of DOT Lebanon s programs Digital Media Literacy Program DML a 2 level digital youth training program delivered in 14 different centers across Lebanon',\n",
       " 'The aim of this study was to identify if acquiring ICT skills through DOT Lebanon s ICT training program improved income generation opportunities after 3 months for DOT Lebanon s beneficiaries enrolled in the training from February 2018 till September 2018',\n",
       " 'All that is needed is an internet connection and access to a laptop which according to the results in this study is not a hindering factor',\n",
       " 'Data was analyzed using Statistical Package for the Social Sciences SPSS version 20',\n",
       " 'The aim of this study was to identify if acquiring ICT skills through DOT Lebanon s ICT training program a local NGO improved income generation opportunities after 3 months of completing the training',\n",
       " 'When evaluating the data qualitatively the results indicate a positive impact on beneficiaries who attended the DOT Lebanon ICT training',\n",
       " 'This is a prospective cohort study describing being exposed to at least 1 income generation opportunity after 3 months from completion of DOT Lebanon s ICT training',\n",
       " 'A limitation that arises with a longitudinal study is the loss of participants over time',\n",
       " 'In fact organizations are now identifying digital skills or computer literacy as one of their core values for employability such as the US Department of Education the US Department of commerce the OECD Program for the International Assessment of Adult Competencies and the European Commission',\n",
       " 'Reduced sample eliminated missing data',\n",
       " 'Reaction time data were available from 985 men who had 2789 evaluations and 480 deaths during follow up',\n",
       " 'The BLSA began to study men in 1958 and women in 1978',\n",
       " 'The resulting corrected data no longer had a significant correlation with the date',\n",
       " 'Data on strength power and tapping time were available from 1969 observations from 709 men',\n",
       " 'Our participants included 1196 men who performed a tapping and or auditory simple respond to a sound and disjunctive respond to a higher pitched sound reaction time tasks while participating in the Baltimore Longitudinal Study of Aging',\n",
       " 'In this study increasing tapping time was found to be associated with increasing mortality',\n",
       " 'Stratifying the data found a significant effect for both tapping and reaction times prior to 10 years of follow up on mortality but not after that period of observation',\n",
       " 'A small systematic drift in the power and strength data was observed by year and was adjusted by regressing power and strength by date to obtain predicted values 4',\n",
       " 'The reaction time data were collected from 1973 through 1991',\n",
       " 'In general the pattern of missing data appeared to be noninformative although more participants might be censored in the missing group',\n",
       " 'No specific health selection criteria were used to screen participants whose data are included in this analysis',\n",
       " 'Descriptive data are expressed as mean 6 standard deviation SD unless otherwise stated',\n",
       " 'Second the data set was reduced to the 1969 visits where all three measurements had been tested',\n",
       " 'The major issue was whether the data collected in women was adequate to actually address the research questions',\n",
       " 'This study addresses whether movement speed or reaction time are risk factors for mortality independent of variables traditionally used to dedscribe sarcopenia including muscle strength muscle power and muscle mass',\n",
       " 'Imputation used imputation to handle missing data',\n",
       " 'Data stratification on time and introduction of a time dependent variable with time expressed as a log function are two approaches that were explored to deal with lack of proportionality 25',\n",
       " 'In preparing this report we found that only 137 women had had tapping data and there was no association between tapping time and mortality',\n",
       " 'Our belief was that the data was underpowered to detect a difference of the magnitude observed in the men',\n",
       " 'The tapping test and the muscle arm cranking and strength measures were not routinely collected on the same visits with 48 missing data',\n",
       " 'Missing data were imputed using the regression approach described by Harrell 27 pages 41 52 using his S function transcan with imputation using fit',\n",
       " 'To explore the relationships between tapping time muscle strength and power on mortality we searched for patterns of missing data 26',\n",
       " 'For this study the only data utilized were from the control state which required tapping into a single circle rather than between two targets',\n",
       " 'The risk associated with longer tapping times was similar across the 4 models for both approaches to handling the missing data suggesting that the impact of tapping is at least in part independent of either strength or power',\n",
       " 'Each plot includes all available data and a LOESS regression line',\n",
       " 'The observations from this study suggest that this movement is controlled somewhat differently than for the arm cranking as each had independent effects on mortality',\n",
       " 'Our participants consisted of 1196 male participants in the Baltimore Longitudinal Study of Aging BLSA 21 who had either tapping or reaction time measurements performed during at least one of the study visits',\n",
       " 'Strength and power were not collected on the same schedule as was tapping in the BLSA producing 48 missing strength and power data',\n",
       " 'The funders had no role in the design of the study in the collection analyses or interpretation of data in the writing of the manuscript or in the decision to publish the results',\n",
       " 'The protocol of the study was approved by the ethics committee LYON B 2002 112B approved 10 February 2003',\n",
       " 'On day 1 of the study characteristics and blood parameters of patients were collected namely age sex weight height body mass index BMI CRP orosomucoid albumin and prealbumin',\n",
       " 'Our data did not show that TEE was increased in acutely or subacutely ill old patients with inflammatory process and malnutrition',\n",
       " 'In two different age groups of 60 74 and 90 years and more from the Louisiana Healthy Aging Study Kim et al',\n",
       " 'We conducted an observational cross sectional study in Lyon Sud University Hospital',\n",
       " 'The nonparametric rank correlation study showed a significant negative correlation between CRPdiff and REE Spearman s coefficient 0',\n",
       " 'In our study FFM seemed to be the main determinant of energy expenditure',\n",
       " 'Despite equilibrium of the energy balance with similar values between energy intakes and energy needs as well as the stability of intakes over the two weeks a significant decrease in weight was observed in our study',\n",
       " 'According to the current state of knowledge ageing is accompanied by a reduction in resting and total energy expenditure TEE 14 but very scarce data are available with precise measurements of TEE',\n",
       " '78 data not shown',\n",
       " 'showed an increase in REE with the number of chronic diseases in a study with healthy volunteers from the Baltimore Longitudinal Study of Aging BLSA cohort 36',\n",
       " 'Measured TEE and mean energy intake were compared using Wilcoxon s test on matched data',\n",
       " 'In a study of hospitalized elderly subjects in acute or rehabilitation care unit REE was found to be very close to ours 39',\n",
       " 'In particular no equation is sufficiently accurate to predict REE in most hospitalized patients 15 18 and there is only very limited data on PAL in older patients 19',\n",
       " 'A more pronounced decline in REE in very old subjects could even be a marker of frailty and sarcopenia as suggested by the results of a recent study 56',\n",
       " 'In a study involving old women from Women s Health and Aging Study WHAS II 44 the average REE was 1119 205 kcal d',\n",
       " 'This suggests that sedentary lifestyle may also explain the decline in REE in addition to the decrease in FFM even though physical activity was not measured in their study 42',\n",
       " 'The primary goal of this work was to measure TEE in acutely ill patients with the DLW method to study very precisely their energy requirements and energy balance',\n",
       " 'In a recent prospective study a Charlson comorbidity index over two was an independent predictor of malnutrition at hospital admission 4',\n",
       " 'Further studies with repeated calorimetry measurements throughout hospitalization along with inflammatory markers would be needed to better study the associations of REE and inflammation',\n",
       " 'However such increase was not observed in the control group of our study which consisted of frail elderly subjects living in nursing homes',\n",
       " 'The limited patient numbers due to the high cost of the DLW technique and cross sectional design are the main limitations of our study',\n",
       " 'In our study very low PAL may also explain the low REE that we observed',\n",
       " 'Characteristics at inclusion of the previous study 20 collected in the control population group B were age weight height BMI REE by indirect calorimetry body composition from total body water TBW measured by H 2 18 O dilution energy intake and MNA',\n",
       " 'For secondary objectives group A was compared to data retrieved from a previous study control group B 20',\n",
       " 'In a longitudinal study TEE was reassessed nearly seven years later in 83 participants whose mean initial age was about 74 years',\n",
       " 'The lack of available data using the DLW method to measure TEE for this population at the time the study was conceived did not allow us to estimate a precise sample for our study',\n",
       " 'This study aimed to obtain a reliable measure of REE and total energy expenditure TEE in older patients hospitalized for an acute episode in order to better assess patients energy requirements and help understand the mechanisms of weight loss in this situation',\n",
       " 'This study was promoted by Lyon University Hospitals Hospices Civils de Lyon France',\n",
       " 'REE was then compared to data from a previous study on aged volunteers from nursing homes who were free of an acute stressor event',\n",
       " 'However it was not envisaged that the relationship between REE and inflammatory status would be explored in this study',\n",
       " 'In another longitudinal study conducted from the Health ABC study cohort on 302 patients aged 70 to 82 years living in a healthy community 51 PAL in only the highest tertile was associated with a reduction in mortality after eight years of follow up while neither TEE nor REE was associated with mortality',\n",
       " 'In Kim s study TEE was also measured by the doubly labeled water method and the authors did not find the same results for TEE with no association between frailty index and TEE 46',\n",
       " 'The precision of the measurement methods used is a strength of our study',\n",
       " 'Data on energy requirements of old and or malnourished acutely ill patients are scarce',\n",
       " 'Nineteen volunteer older patients age 65 years hospitalized in acute or rehabilitation care unit with an acute condition malnutrition and an ongoing inflammatory process were included study group hereafter referred to as group A',\n",
       " 'In the study by Fabbri et al',\n",
       " 'To demonstrate the importance of using perturbation for generating sufficient samples we further evaluated the performance of our statistical model without perturbation and the average prediction error on the same validation data increases to 15',\n",
       " 'We show the experimental results on both real and simulated data in Section III',\n",
       " 'This data set consists of 40 subjects each manually labeled with 54 regions of interest ROIs',\n",
       " 'The LONI LPBA40 data set was used',\n",
       " '7 on the validation data',\n",
       " 'Upon the LONI LPBA40 data set with a size of 220 220 184 used in the previous section we compared the average computation cost of our proposed method incorporating HAMMER diffeomorphic demons FNIRT ART and SyN with that of the original counterpart',\n",
       " 'Here we further evaluate the generalization performance of our proposed learning based framework by training the correlation model using one data set and applying to it to another data set',\n",
       " 'The baseline images are selected from the Baltimore Longitudinal Study of Aging BLSA 48 data set',\n",
       " 'In all previous experiments we all used a subgroup of the same data set for training and performed testing based on the remaining data',\n",
       " 'Specifically we used 40 images from the LONI LPBA data set for training and 16 images from the NIREP with 32 Fig',\n",
       " 'Since the validation data were not used for training the error in estimating its deformation coefficients is larger than the training error which is reasonable',\n",
       " '24 presented a method for learning deformation statistics from noisy and incomplete data',\n",
       " 'The LONI LPBA40 data set is used for this experiment',\n",
       " 'Steps 4 and 5 involve constructing the brain appearance model based on the data set',\n",
       " 'A similar experiment has been conducted using RABBIT reported in 36 which achieves about 68 overlap ratio for NIREP data',\n",
       " 'However the difference between their experiment and ours is that in their experiment all training images Open Access Series of Imaging Studies database 50 are aligned to a template selected from the test data set NIREP database',\n",
       " '5 on the training data and 5',\n",
       " 'Parameter is used to control the width of the insensitive zone that penalizes the training data outside this zone']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_batch = NERData.get_sentences(ner_data)\n",
    "text_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f4ab69e2-b38a-4094-8665-88db44a388b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "61b57294-554d-45d9-adff-9b46127aab01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9221, 2193]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize('validate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "38826a4a-a84f-4a40-b181-67e0e80a11ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] validate [SEP]'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer('validate')['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6123503c-649c-4ba8-8bb5-45d820ec4a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tokenizer(['validate', 'I go out today', 'Something in the water looks at me'], \n",
    "                padding=True, max_length=4, truncation=True, return_overflowing_tokens=True)\n",
    "ids = res['input_ids']\n",
    "overflows = res['overflowing_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3a3d6608-cfd0-48a7-83cb-546f6eab1085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] validate [SEP]', '[CLS] I go [SEP]', '[CLS] Something in [SEP]']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "92530c86-24aa-4214-90f1-7a741cd8a6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'today out', 'me at looks water the']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(overflows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1f1a9db7-24e2-4e35-ae12-348edf2de077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] validate [SEP]'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([101, 9221, 2193, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b9f8c3bc-f58e-4155-8004-c38f334f374c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 9221, 2193, 102]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('validate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b8d6ba53-2d33-484c-85ce-15c22249e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(tupled_sentence):\n",
    "    tokenized_sentence = []\n",
    "\n",
    "    for (word, label) in tupled_sentence:\n",
    "\n",
    "        # Tokenize the word\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        \n",
    "        # Repeat the label for words that are broken up into several tokens\n",
    "        repeated_label = [label]*len(tokenized_word)\n",
    "        \n",
    "        # Add the tokenized word and its label to the final tokenized word list\n",
    "        tokenized_sentence.extend(zip(tokenized_word, repeated_label))\n",
    "\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f508c82e-c0c2-412e-9671-575777d0e0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'O'),\n",
       " ('research', 'O'),\n",
       " ('study', 'O'),\n",
       " ('comes', 'O'),\n",
       " ('to', 'O'),\n",
       " ('valid', 'O'),\n",
       " ('##ate', 'O'),\n",
       " ('and', 'O'),\n",
       " ('q', 'O'),\n",
       " ('##uant', 'O'),\n",
       " ('##ify', 'O'),\n",
       " ('a', 'O'),\n",
       " ('number', 'O'),\n",
       " ('of', 'O'),\n",
       " ('assumptions', 'O'),\n",
       " ('in', 'O'),\n",
       " ('the', 'O'),\n",
       " ('Lebanese', 'O'),\n",
       " ('context', 'O')]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data = [\n",
    "   tokenize_and_preserve_labels(sentence)\n",
    "    for sentence in ner_data\n",
    "]\n",
    "ner_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b0962689-ca03-4a74-ac4b-2689a5dd85cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_end_tokens(tupled_sentence):\n",
    "    tupled_sentence.insert(0, ('[CLS]', 'O'))\n",
    "    tupled_sentence.append(('[SEP]', 'O'))\n",
    "    return tupled_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f3ee0f3f-895b-456b-bef1-732b67a55ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', 'O'),\n",
       " ('This', 'O'),\n",
       " ('research', 'O'),\n",
       " ('study', 'O'),\n",
       " ('comes', 'O'),\n",
       " ('to', 'O'),\n",
       " ('valid', 'O'),\n",
       " ('##ate', 'O'),\n",
       " ('and', 'O'),\n",
       " ('q', 'O'),\n",
       " ('##uant', 'O'),\n",
       " ('##ify', 'O'),\n",
       " ('a', 'O'),\n",
       " ('number', 'O'),\n",
       " ('of', 'O'),\n",
       " ('assumptions', 'O'),\n",
       " ('in', 'O'),\n",
       " ('the', 'O'),\n",
       " ('Lebanese', 'O'),\n",
       " ('context', 'O'),\n",
       " ('[SEP]', 'O')]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data = [\n",
    "   add_start_end_tokens(sentence)\n",
    "    for sentence in ner_data\n",
    "]\n",
    "ner_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c9159c0a-ee47-421a-b18e-27016023eb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'This',\n",
       " 'research',\n",
       " 'study',\n",
       " 'comes',\n",
       " 'to',\n",
       " 'valid',\n",
       " '##ate',\n",
       " 'and',\n",
       " 'q',\n",
       " '##uant',\n",
       " '##ify',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'assumptions',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Lebanese',\n",
       " 'context',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get only sentences, not labels\n",
    "tokenized_sentences = [[token_label_tuple[0] for token_label_tuple in sent] for sent in ner_data ]\n",
    "\n",
    "# Get only labels, not sentences\n",
    "labels = [[token_label_tuple[1] for token_label_tuple in sent] for sent in ner_data ]\n",
    "\n",
    "tokenized_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b25ac58c-6279-41c3-9b03-d9ad62b16c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(tokenized_sentences, labels):\n",
    "    # Note that this implicitly converts to an array of objects (strings)\n",
    "    \n",
    "    padded_sentences = pad_sequences(\n",
    "        tokenized_sentences, \n",
    "        value='[PAD]', \n",
    "        dtype=object, \n",
    "        maxlen=MAX_LENGTH, \n",
    "        truncating='post', \n",
    "        padding='post')\n",
    "\n",
    "    padded_labels = pad_sequences(\n",
    "        labels, \n",
    "        value='O', \n",
    "        dtype=object, \n",
    "        maxlen=MAX_LENGTH, \n",
    "        truncating='post', \n",
    "        padding='post')\n",
    "    \n",
    "    return padded_sentences, padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d7626512-9411-44d3-bd4a-420204de348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sentences, padded_labels = add_padding(tokenized_sentences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b71f2eca-4260-4cc7-9e69-ffdde9ddc5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[CLS]', 'This', 'research', 'study', 'comes', 'to', 'valid',\n",
       "       '##ate', 'and', 'q', '##uant', '##ify', 'a', 'number', 'of',\n",
       "       'assumptions', 'in', 'the', 'Lebanese', 'context', '[SEP]',\n",
       "       '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]',\n",
       "       '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]',\n",
       "       '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]',\n",
       "       '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]',\n",
       "       '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]',\n",
       "       '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]',\n",
       "       '[PAD]'], dtype=object)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "025c1272-161a-4a02-8477-21f0c52eb7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1188, 1844, 2025, 2502, 1106, 9221, 2193, 1105, 186, 27280, 6120, 170, 1295, 1104, 19129, 1107, 1103, 12772, 5618, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Convert to integer ids\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(text) for text in padded_sentences]\n",
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f79a9f3d-1a8a-4e57-a03e-c815cd47c7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152]\n"
     ]
    }
   ],
   "source": [
    "# Convert to integer ids\n",
    "tags = [tokenizer.convert_tokens_to_ids(text) for text in padded_labels]\n",
    "print(tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b9fae980-efdb-4b1c-9cd8-c12ffadecedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([64]), array([64]))"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All of the same length?\n",
    "np.unique([len(sent) for sent in padded_sentences]), np.unique([len(sent) for sent in padded_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "74e0e579-697d-4d7f-a28f-21e50a997a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] This research study comes to validate and quantify a number of assumptions in the Lebanese context [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] According to a study completed by UNDP 2016 96 of semi skilled workers in the Agro food industry suffer from basic ICT skills weaknesses 89 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can decode correctly?\n",
    "tokenizer.batch_decode(input_ids[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6ccc3cd9-92e5-4e44-9d18-b7573376e891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Get attention mask\n",
    "# QUESTION: Also ignore [CLS] and [SEP] tokens?\n",
    "\n",
    "def get_attention_mask(input_ids, ignore_tokens=[0,101,102]):\n",
    "    return [[float(token not in ignore_tokens) for token in sent] \n",
    "                for sent in input_ids]\n",
    "attention_mask = get_attention_mask(input_ids)\n",
    "print(attention_mask[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edecbae1",
   "metadata": {},
   "source": [
    "# Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaebf207",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc9e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models are initialized in eval mode by default. We can call model.train() to put it in train mode.\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe3d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41c92f07",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CLS]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[UNK]',\n",
       "  '[SEP]'],\n",
       " 102)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text_batch)))\n",
    "tokens, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "305cf12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee90f23320bc41578bca91d11ca094a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ae3042dc7c492782752a2faa67749e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e952394c9f99420da8215ba0af3402e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True)\n",
    "inputs = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3785f2b6-1697-46a0-9cc1-74e95c700500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101,  100, 2470,  ...,    0,    0,    0],\n",
       "        [ 101,  100, 2000,  ...,    0,    0,    0],\n",
       "        [ 101,  100, 3433,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101,  100, 1996,  ...,    0,    0,    0],\n",
       "        [ 101, 1019, 2006,  ...,    0,    0,    0],\n",
       "        [ 101,  100, 2003,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc99c95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19,\n",
       " 'After acquisition of this last DKI dataset at 8 months of age mice were then sacrificed for histological analysis')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ner_data.data[0]), text_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([1,0]).unsqueeze(0)\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0a6d14aafaa1289692eb21b35fbb4fde63fb8f6bc0d072ac5f475793e83354f30",
   "display_name": "Python 3.8.5 64-bit ('nlp')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}