{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ocN7hWLOMUuU",
   "metadata": {
    "id": "ocN7hWLOMUuU"
   },
   "outputs": [],
   "source": [
    "# built in\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "# append to path to allow relative imports\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QLrmqlgAKwVZ",
   "metadata": {
    "id": "QLrmqlgAKwVZ"
   },
   "outputs": [],
   "source": [
    "!pip install rich transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9A3602YGJ08W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9A3602YGJ08W",
    "outputId": "83ac6eed-ad74-4de9-b9c3-019190a53fe7"
   },
   "outputs": [],
   "source": [
    "# In case you need to connect your repository\n",
    "! git clone https://<gh-token>@github.com/verrannt/show-us-the-data.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZIZ5H-JbMQEr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZIZ5H-JbMQEr",
    "outputId": "0a76a8d2-d907-46fe-e8c1-686aec4613f4"
   },
   "outputs": [],
   "source": [
    "# Navigate to it\n",
    "os.chdir('show-us-the-data/')\n",
    "! git pull\n",
    "os.chdir('src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TeU7JwCaLoiA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TeU7JwCaLoiA",
    "outputId": "23965684-f9fa-4706-9709-42ac19bf8bef"
   },
   "outputs": [],
   "source": [
    "# Mount drive and set path to data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "GDRIVE_DATA_PATH = 'gdrive/MyDrive/Datasets/show-us-the-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c70af",
   "metadata": {
    "id": "998c70af"
   },
   "outputs": [],
   "source": [
    "# 3rd party\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rich.console import Console\n",
    "from rich.progress import track\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForTokenClassification, BertModel, AdamW, BertTokenizerFast\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# own\n",
    "from utils.data.preproc import Pipeline, PipelineConfigs\n",
    "from utils.data.parse import ParseUtils\n",
    "from utils.generic import timer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738af29f",
   "metadata": {
    "id": "738af29f"
   },
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15b7439",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d15b7439",
    "outputId": "5a7535e9-3304-4ce0-cbf3-6506e3c941f0"
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join(\n",
    "    os.path.abspath('../../'), # Root of project\n",
    "    #GDRIVE_DATA_PATH\n",
    "    'data/coleridgeinitiative-show-us-the-data/' # Data folder\n",
    ")\n",
    "\n",
    "configs = PipelineConfigs(\n",
    "    DATA_PATH = data_path,\n",
    "    MAX_LENGTH = 64,\n",
    "    OVERLAP = 20,\n",
    "    MAX_SAMPLE = None,\n",
    "    SAVE = True,\n",
    "    EXTRACTED_FILENAME = 'train_ner.data',\n",
    "    TOKENIZED_FILENAME = 'train_ner.data.tokenized',\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0: \n",
    "    torch.cuda.get_device_name(0)\n",
    "    print('Running on GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c602f89",
   "metadata": {
    "id": "8c602f89"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9b9627",
   "metadata": {
    "id": "ec9b9627"
   },
   "outputs": [],
   "source": [
    "input_ids, tags, attention_masks = pipeline.load_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OCvlBn8RNlrf",
   "metadata": {
    "id": "OCvlBn8RNlrf"
   },
   "outputs": [],
   "source": [
    "#input_ids, tags, attention_masks = input_ids[:500], tags[:500], attention_masks[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00abbd4f",
   "metadata": {
    "id": "00abbd4f"
   },
   "outputs": [],
   "source": [
    "# Intermezzo fix tag labels\n",
    "tag2id = {t:i for i, t in enumerate(np.unique(tags))}\n",
    "tags = [[tag2id[tag] for tag in sent] for sent in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181adc83",
   "metadata": {
    "id": "181adc83"
   },
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(\n",
    "    input_ids, \n",
    "    tags,\n",
    "    random_state=2018, \n",
    "    test_size=0.1\n",
    ")\n",
    "\n",
    "tr_masks, val_masks, _, _ = train_test_split(\n",
    "    attention_masks, \n",
    "    input_ids,\n",
    "    random_state=2018, \n",
    "    test_size=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c242ccc",
   "metadata": {
    "id": "6c242ccc"
   },
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs).to(device)\n",
    "val_inputs = torch.tensor(val_inputs).to(device)\n",
    "tr_tags = torch.tensor(tr_tags).to(device)\n",
    "val_tags = torch.tensor(val_tags).to(device)\n",
    "tr_masks = torch.tensor(tr_masks).to(device)\n",
    "val_masks = torch.tensor(val_masks).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8699f74",
   "metadata": {
    "id": "a8699f74"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb07c8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9eb07c8c",
    "outputId": "8801f3fa-9192-4a77-cd80-49a93cb8fb7a"
   },
   "outputs": [],
   "source": [
    "len(tr_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aebb30",
   "metadata": {
    "id": "48aebb30"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ce773d",
   "metadata": {
    "id": "39ce773d"
   },
   "source": [
    "# Using `BertModel` to feed embeddings into separate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f04cbcf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f04cbcf",
    "outputId": "121c2c3e-d3d4-4010-9ebb-6a5c4d00ac0e"
   },
   "outputs": [],
   "source": [
    "# Get bert model. This will output raw hidden states\n",
    "model = BertModel.from_pretrained(\n",
    "    'bert-base-cased',\n",
    "    num_labels = 3\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0b640f",
   "metadata": {
    "id": "ca0b640f"
   },
   "outputs": [],
   "source": [
    "# Create one layer linear classifier to be used on top\n",
    "# of hidden states\n",
    "cls = nn.Linear(\n",
    "    model.config.hidden_size, \n",
    "    model.config.num_labels\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460448fd",
   "metadata": {
    "id": "460448fd"
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "def create_batched_embeddings(\n",
    "    model,  # The HuggingFace BertModel to create the embeddings\n",
    "    inputs, # The inputs to be fed to the model\n",
    "    masks,  # Attention masks for the inputs (same shape as inputs)\n",
    "    batch_size:int=32,\n",
    "    size_limit:int=10000,\n",
    "    store_dir:str='temp/'\n",
    "):\n",
    "    \"\"\"\n",
    "    Feed batches of size `batch_size` into the provided model from provided \n",
    "    inputs and masks, and return the resulting embeddings as a `torch.tensor`\n",
    "    \"\"\"\n",
    "    _total = inputs.shape[0]\n",
    "    # Drop the last data points if they don't fit with the batch size\n",
    "    _total = (_total // batch_size) * batch_size\n",
    "    print(f'Computing embeddings for {_total} items')\n",
    "\n",
    "    # If the size gets too large\n",
    "    if _total > size_limit:\n",
    "        _backup_to_file = True\n",
    "        _save_count = 1\n",
    "        if not os.path.exists(store_dir):\n",
    "            os.mkdir(store_dir)\n",
    "    else:\n",
    "        _backup_to_file = False\n",
    "\n",
    "    def _save_job(file,count,store_dir):\n",
    "        print(f\"Saving at {count} items ... \", end = '')\n",
    "        ParseUtils.save_outputs(\n",
    "            file, \n",
    "            store_dir, \n",
    "            'embeddings.{}'\n",
    "                .format(count)\n",
    "        )\n",
    "        print(\"Done.\")\n",
    "\n",
    "    # Collect embeddings in Python list\n",
    "    _embeds = []\n",
    "\n",
    "    for i in track(range(0, _total, batch_size),\n",
    "                   description='Creating embeddings'):\n",
    "        with torch.no_grad():\n",
    "            _out = model(\n",
    "                tr_inputs[i:i+batch_size], attention_mask=tr_masks[i:i+batch_size]\n",
    "            ).last_hidden_state.detach().cpu().numpy()\n",
    "        \n",
    "        _embeds.append(_out)\n",
    "\n",
    "        if _backup_to_file and i >= _save_count * size_limit:\n",
    "            _save_count += 1\n",
    "            _save_job(np.array(_embeds, dtype=np.float16), i, store_dir)\n",
    "            # Free memory\n",
    "            del _embeds\n",
    "            _embeds = []\n",
    "    \n",
    "    if _backup_to_file:\n",
    "        _save_job(_embeds, _total, store_dir)\n",
    "        # Free memory\n",
    "        del _embeds\n",
    "        _embeds = []\n",
    " \n",
    "    return _embeds\n",
    "\n",
    "def create_batched_tensor(\n",
    "    tensor,\n",
    "    batch_size:int=32,\n",
    "):\n",
    "    \"\"\"\n",
    "    From any given tensor, split the first dimension into batches of size\n",
    "    `batch_size` and return as a `torch.tensor`\n",
    "    \"\"\"\n",
    "    total = tensor.shape[0] \n",
    "    # Drop the last data points if they don't fit with the batch size\n",
    "    total = (total // batch_size) * batch_size\n",
    "    \n",
    "    return torch.tensor([\n",
    "        tensor[i:i+batch_size].cpu().numpy()\n",
    "        for i in track(range(0, total, batch_size),\n",
    "            description='Creating batches')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrZCFJkOiMgk",
   "metadata": {
    "id": "wrZCFJkOiMgk"
   },
   "outputs": [],
   "source": [
    "!rm -rf ../../temp\n",
    "os.mkdir('../../temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ae02b",
   "metadata": {
    "id": "022ae02b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emb_data = create_batched_embeddings(\n",
    "    model, \n",
    "    tr_inputs, \n",
    "    tr_masks, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    size_limit=2000,\n",
    "    store_dir = 'temp/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ilZx9a1csVYj",
   "metadata": {
    "id": "ilZx9a1csVYj"
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "def compress(tar_file, members):\n",
    "    \"\"\"\n",
    "    Adds files (`members`) to a tar_file and compress it\n",
    "    \"\"\"\n",
    "    # open file for gzip compressed writing\n",
    "    tar = tarfile.open(tar_file, mode=\"w:gz\")\n",
    "    # with progress bar\n",
    "    # set the progress bar\n",
    "    progress = tqdm(members)\n",
    "    for member in progress:\n",
    "        # add file/folder/link to the tar file (compress)\n",
    "        tar.add(member)\n",
    "        # set the progress description of the progress bar\n",
    "        progress.set_description(f\"Compressing {member}\")\n",
    "    # close the file\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WGu5xzThsWeE",
   "metadata": {
    "id": "WGu5xzThsWeE"
   },
   "outputs": [],
   "source": [
    "#compress(\"compressed.10016.tar.gz\", [\"../../temp/train_ner.data.tokenized.bert-base-cased.embeddings.bs32.10016\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3940b8",
   "metadata": {
    "id": "4b3940b8"
   },
   "outputs": [],
   "source": [
    "labels = create_batched_tensor(tr_tags, batch_size=BATCH_SIZE)\n",
    "masks = create_batched_tensor(tr_masks, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149920ba",
   "metadata": {
    "id": "149920ba"
   },
   "outputs": [],
   "source": [
    "emb_data.shape, labels.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98592cc3",
   "metadata": {
    "id": "98592cc3"
   },
   "outputs": [],
   "source": [
    "NUM_LABELS = model.config.num_labels\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 3e-5\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(cls.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e65ad6",
   "metadata": {
    "id": "55e65ad6"
   },
   "outputs": [],
   "source": [
    "loss_values, accuracies, f1_scores = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fJu_2urRJkEX",
   "metadata": {
    "id": "fJu_2urRJkEX",
    "outputId": "45c0d013-0c13-40cf-9683-8a64b0f913e8"
   },
   "outputs": [],
   "source": [
    "total = emb_data.shape[0]\n",
    "\n",
    "USE_MASK = True\n",
    "\n",
    "for epoch in range(1,EPOCHS+1):\n",
    "    \n",
    "    # Collect metrics for all batches\n",
    "    losses, accs, f1s = [], [], []\n",
    "    \n",
    "    for step, (X, y) in enumerate(zip(emb_data, labels)):\n",
    "                \n",
    "        # Compute prediction\n",
    "        logits = cls(X)\n",
    "        # Flatten over batch and sequence\n",
    "        flat_logits = logits.view(-1, NUM_LABELS)\n",
    "        \n",
    "        # Compute loss\n",
    "        if USE_MASK:\n",
    "            loss_mask = masks[step].view(-1) == 1\n",
    "            flat_y = torch.where(\n",
    "                loss_mask, y.view(-1), torch.tensor(loss_fn.ignore_index).type_as(y)\n",
    "            )\n",
    "            loss = loss_fn(flat_logits, flat_y)\n",
    "        else:\n",
    "            flat_y = y.view(-1)\n",
    "            loss = loss_fn(flat_logits, flat_y) \n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Detach and argmax\n",
    "        flat_logits = np.argmax(flat_logits.detach(), axis=1)\n",
    "        flat_y = flat_y.detach()\n",
    "        \n",
    "        # Compute metrics\n",
    "        losses.append(loss.item())        \n",
    "        accs.append(accuracy_score(flat_logits, flat_y))\n",
    "        f1s.append(f1_score(flat_logits, flat_y, average='weighted'))\n",
    "        \n",
    "    loss = np.mean(losses)\n",
    "    acc = np.mean(accs)\n",
    "    f1 = np.mean(f1s)\n",
    "        \n",
    "    loss_values.append(loss)\n",
    "    accuracies.append(acc)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print('[ Epoch {:3}/{} ]   Loss: {:.5f}   Acc: {:.5f}   F1: {:.5f}'.format(epoch, EPOCHS, loss, acc, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b2714",
   "metadata": {
    "id": "0d1b2714",
    "outputId": "854951ed-f2d1-4db8-9368-bf00fc6aa47c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.xlabel('Epochs')\n",
    "plt.plot(loss_values, label='Loss')\n",
    "plt.plot(accuracies, label='Accuracy')\n",
    "plt.plot(f1_scores, label='F1 Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c126b6",
   "metadata": {
    "id": "16c126b6"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25985ded",
   "metadata": {
    "id": "25985ded"
   },
   "source": [
    "# Using `BertForTokenClassification` with possible finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaebf207",
   "metadata": {
    "id": "eaebf207"
   },
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\n",
    "    'bert-base-cased',\n",
    "    num_labels = 3, # Hardcode for now\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models are initialized in eval mode by default. We can call model.train() to put it in train mode.\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614d2c2a",
   "metadata": {
    "id": "614d2c2a"
   },
   "outputs": [],
   "source": [
    "# Full finetuning to tune all model parameters\n",
    "# Otherwise, only train classifier\n",
    "FULL_FINETUNING = True\n",
    "\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d18d42",
   "metadata": {
    "id": "95d18d42"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "LEARNING_RATE = 3e-5\n",
    "EPSILON = 1e-8\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=LEARNING_RATE,\n",
    "    eps=EPSILON\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6d4e6",
   "metadata": {
    "id": "8ca6d4e6"
   },
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ccf07f",
   "metadata": {
    "id": "88ccf07f"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5324d1",
   "metadata": {
    "id": "1a5324d1"
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc626d0",
   "metadata": {
    "id": "5fc626d0"
   },
   "outputs": [],
   "source": [
    "## Store the average loss after each epoch so we can plot them.\n",
    "loss_values, validation_loss_values = [], []\n",
    "n_train_samples = len(train_dataloader)\n",
    "n_val_samples = len(valid_dataloader)\n",
    "\n",
    "for eidx in range(EPOCHS):\n",
    "\n",
    "    print(f'Epoch {eidx}/{EPOCHS}')\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        print(f'Step {step}/{n_train_samples}')\n",
    "\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Always clear any previously calculated gradients before performing a backward pass.\n",
    "        model.zero_grad()\n",
    "        # forward pass\n",
    "        # This will return the loss (rather than the model output)\n",
    "        # because we have provided the `labels`.\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "        # get the loss\n",
    "        loss = outputs[0]\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        total_loss += loss.item()\n",
    "        # Clip the norm of the gradient\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "    # Reset the validation loss for this epoch.\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "\n",
    "        print(f'Step {step}/{n_val_samples}')\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients,\n",
    "        # saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have not provided labels.\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "\n",
    "    eval_loss = eval_loss / len(valid_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
    "    valid_tags = [tag_values[l_i] for l in true_labels\n",
    "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
    "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
    "    print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857d7bb",
   "metadata": {
    "id": "7857d7bb"
   },
   "outputs": [],
   "source": [
    "test_outputs = model(tr_inputs[:32], attention_mask=tr_masks[:32], labels=tr_tags[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3c2a9",
   "metadata": {
    "id": "9ec3c2a9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "25985ded"
   ],
   "name": "BertTest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}