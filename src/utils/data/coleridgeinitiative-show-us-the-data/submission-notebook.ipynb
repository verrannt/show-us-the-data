{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\nimport json\nimport re\nimport os\nimport pandas as pd\nimport pickle\nfrom tqdm import tqdm,trange\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score,accuracy_score\nimport torch\nfrom transformers import BertForTokenClassification, AdamW, BertTokenizerFast\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n#from rich.console import Console\n#from rich.progress import track\nfrom tqdm import tqdm\nfrom transformers import BertTokenizerFast","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"test_csv_path = '../input/coleridgeinitiative-show-us-the-data/sample_submission.csv'\ntest_path  = '../input/coleridgeinitiative-show-us-the-data/test'\nmodel_path = '../input/bert-show-data-weights'\ntokenizer_path = '../input/huggingface-bert/bert-base-cased'","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class SubmitPred:\n    def __init__(self,test_csv_path,test_path,model_path,tokenizer_path):\n        self.test_csv_path = test_csv_path\n        self.test_path = test_path\n        self.model = BertForTokenClassification.from_pretrained(model_path,num_labels=3,output_attentions=False,output_hidden_states=False)\n        self.tokenizer = BertTokenizerFast.from_pretrained(tokenizer_path,do_lower_case = False)\n        \n    \n    def retrieve_label(self,sentence,model):\n        tokenized_sentence = self.tokenizer.encode(sentence)\n        input_ids = torch.tensor([tokenized_sentence]).cuda()\n        with torch.no_grad():\n            output = model(input_ids)\n        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n        predicted_string = np.array(tokenized_sentence)[np.where(label_indices!=2)[1]]\n        predicted_string = self.tokenizer.decode(predicted_string)\n        return predicted_string\n    \n    def load_submission(self,test_path):\n        self.sample_submission = pd.read_csv(test_path)\n    def predict_from_text(self,sentence_list,model):\n        all_preds = []\n        for sentence in sentence_list:\n            preds = self.retrieve_label(sentence,model)\n            if preds!= '':\n                if not preds.startswith('#'):\n                    all_preds.append(preds)\n        print(all_preds)\n        return \"|\".join(all_preds)\n    \n    @staticmethod\n    def shorten_sentences(sentences, max_len, overlap):\n        short_sentences = []\n        for sentence in sentences:\n            words = sentence.split()\n            if len(words) > max_len:\n                for p in range(0, len(words), max_len - overlap):\n                    short_sentences.append(' '.join(words[p:p + max_len]))\n            else:\n                short_sentences.append(sentence)\n        return short_sentences\n\n    @staticmethod\n    def clean_training_text(txt):\n        \"\"\"\n        similar to the default clean_text function but without lowercasing.\n        \"\"\"\n        txt = re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n\n\n        return txt\n\n    def extract_and_clean(self,list_sentences):\n        list_sentences = [self.clean_training_text(sentence) for sentence in list_sentences]\n        list_sentences = self.shorten_sentences(list_sentences,max_len=64,overlap=20)\n        sentences = [sentence for sentence in list_sentences if len(sentence) > 10] \n        return sentences\n        \n    def parse_json(self,json_id, to_return):\n        path_to_json = os.path.join(self.test_path, (json_id+'.json'))\n        heading = []\n        content = []\n        print(path_to_json)\n        with open(path_to_json, 'r') as f:\n            json_decode = json.load(f)\n            for data in json_decode:\n                heading.append(data.get('section_title'))\n                content.append(data.get('text'))\n        if to_return == \"heading\":\n            all_heading = \",\".join(heading)\n            return all_heading\n        if to_return == \"content\":\n            all_content = \".\".join(content)\n            return all_content\n    \n    def generate_submission(self):\n        self.load_submission(self.test_csv_path)\n        self.model.cuda()\n        \n        print(self.sample_submission)\n        self.sample_submission['content_list'] = self.sample_submission.apply(lambda x:self.parse_json(x.Id,\"content\").split(\".\"),axis=1)\n        self.sample_submission['content_list'] = self.sample_submission.apply(lambda x:self.extract_and_clean(x.content_list),axis=1)\n        self.sample_submission['PredictionString'] = self.sample_submission.apply(lambda x:self.predict_from_text(x.content_list,self.model),axis=1)\n        self.sample_submission.drop('content_list',axis=1)\n        self.sample_submission.to_csv(f'submission.csv', index=False)\n    \n    def print_submission(self):\n        print(self.sample_submission.head())\n        ","metadata":{"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"sub_pred = SubmitPred(test_csv_path,test_path,model_path,tokenizer_path)","metadata":{"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"sub_pred.generate_submission()","metadata":{"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"                                     Id  PredictionString\n0  2100032a-7c33-4bff-97ef-690822c43466               NaN\n1  2f392438-e215-4169-bebf-21ac4ff253e1               NaN\n2  3f316b38-1a24-45a9-8d8c-4e05a42257c6               NaN\n3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60               NaN\n../input/coleridgeinitiative-show-us-the-data/test/2100032a-7c33-4bff-97ef-690822c43466.json\n../input/coleridgeinitiative-show-us-the-data/test/2f392438-e215-4169-bebf-21ac4ff253e1.json\n../input/coleridgeinitiative-show-us-the-data/test/3f316b38-1a24-45a9-8d8c-4e05a42257c6.json\n../input/coleridgeinitiative-show-us-the-data/test/8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60.json\n['[CLS]', 'Co Genomi ConsortiumGE', '[CLS]', 'Science Genetic Association Consortium', 'Co Genomioh', 'Il', 'MD', 'MD', 'Gens', 'Met', 'GenomeGENT', 'Genome', 'Childhood Intelligence ConsortiumGE', 'Heart and A Research Genomi Eemiology', 'D', 'Neuro Imaging', 'NC and A Research Genomi Eemiology']\n['Eight Italy', 'Progress International reading StudyI Program International Student AssessmentIS International Mathematics', 'International reading Licy StudyI Program International StudentIS International Mathematics and Science StudyIM', 'IN', 'G', 'G 8', 'Italy', 'IntroductionductI', 'International readingcy StudyI Program International Student Assessment Mathematics and ScienceIM', 'Education', 'Eight Italy', 'Doctor', 'IN', 'U', 'NC NC Integrated Posty Education NC', 'ID', 'PIIMSSIM', 'I Mu', 'IN', 'NC', 'NC', 'Italy', 'International', 'International', 'Foreign', 'International Mathematics and ScienceIM', '8', 'Economic', 'International So Index of Status', 'So Index', 'ISEI', 'International reading StudyI', 'G', '[CLS]', 'International Mathematics and Science StudyIM', '8', 'Italy In Teachers', 'PP', 'International Major Field StudyCE', '60', 'International Mathematics and Science StudyIMSS', 'International Mathematics and Science StudyIMSS', 'Program International StudentIS', '[CLS]IS', 'Program International StudentIS', 'G 8', 'G', 'G', 'ItalyDIOnAIME ADU', '8', 'Programs', '8', 'Academic', 'In', 'reader', 'Education', 'Statistics Sciences', 'Advanced Advanced', 'General Advanced AS Advanced', '19 Diploma', '[CLS] programs', 'Diploma', 'DE', 'Doctor', 'Ava', 'Doctor', 'Di', 'Everytat', 'Doctor', 'NC', 'Education', 'Statistics Sciences', 'Honors', 'honors']\n['Parkmp Park', 'N', 'Outer', 'nor', 'NC Study', 'Outer', 'Outer', 'Grave', 'Program', '[CLS]DA', 'US DS', 'Arc', 'DS', 'NH', 'At Administrationdet Survey Re DivisionDAplain', 'Arc', 'HDA', 'DS', 'Do', 'Do', 'DS', 'US DSAS', 'Wildlife Ha', 'Do', 'Sur In Map SLR', 'NO', 'Weather NH', 'LiDA', 'N Map Program', 'North Map Program', 'N', 'DE', 'Hydeplain Program Arc', 'Jo', 'Sal', 'Sur Vulner', 'NC Science', 'Life', 'NC', '[CLS] Eion DC', 'E', 'Mir No', 'Mir', 'NC NC Study', 'Fund', 'Mir No No', 'No', 'Noment Fund No', 'Erosion', 'Erosion', 'Research Center', 'Hyde', 'Hyde Park', 'Insurance ProgramFI', 'DFI', 'DFI InformationRI', 'NFI', 'US', 'Ha', 'Ha', 'N', 'Map Program', 'NC Arc APNE Information Coord NC', 'NC Sea Grant Policy Fellowship', 'N', 'Hazard Vulnerability Sur National NW', 'N Districts', 'Outerhore', 'NC', 'N', 'DC', 'Insurance', 'One', 'N', 'Centercie', 'Jam Haven', 'Old', 'V IndexVI', 'V', 'D', 'Sal', 'Keeper Am']\n['[CLS] Information Resource IRI Consumer Network USD Economic Research Consumer Ex Bureau', 'IRI', 'Nielsen', 'National Bureau Research', 'IRI', 'Dam', 'Marketing Institute', 'Marketing', 'Agriculture', 'IRI IRINP', 'USD', 'IRI', 'Bureauverty', 'USD Food Access Research', 'USD Rural Urban', 'Ex', 'USD National Health and Nutrition USD', 'USD', 'Agriculture', 'NLIIII', 'Agriculture', 'NLI', 'IRI', 'IRINPII', 'IRII', 'IRII', 'IRII', 'Agriculture', 'CE IRI', 'IRI', 'IRI', 'IRI', 'CE IRI', 'IRINP', 'IRII']\n","output_type":"stream"}]}]}